{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "451e3a9c",
   "metadata": {},
   "source": [
    "# 👁️ Level 3.1: The Image Recognition Quest\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/ai-mastery-from-scratch/blob/main/notebooks/phase_3_practical_ai_systems/3.1_image_recognition_quest.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **The Challenge**\n",
    "**Can AI recognize handwritten digits?**\n",
    "\n",
    "Welcome to the real world of AI! Today we're tackling one of the most famous challenges in machine learning - teaching a computer to recognize handwritten digits. This is where theory meets practice, and where you'll build your first truly practical AI system.\n",
    "\n",
    "### **What You'll Discover:**\n",
    "- 🖼️ How AI \"sees\" and processes images\n",
    "- 🧠 Scaling neural networks to handle real data\n",
    "- 📊 Building your first computer vision system\n",
    "- ✨ The magic moment when AI learns to see patterns\n",
    "\n",
    "### **What You'll Build:**\n",
    "A digit recognition system that can identify handwritten numbers with impressive accuracy!\n",
    "\n",
    "### **The Journey Ahead:**\n",
    "1. **The Dataset Explorer** - Meet the famous MNIST dataset\n",
    "2. **The Vision Preprocessor** - Prepare images for AI consumption  \n",
    "3. **The Pattern Learner** - Build a neural network from scratch\n",
    "4. **The Recognition System** - Put it all together for real-time recognition\n",
    "5. **The Performance Analyzer** - Understand what your AI learned\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 **Setup & Installation**\n",
    "\n",
    "*Run the cells below to set up your environment. This works in both Google Colab and local Jupyter notebooks.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2973e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Install Required Packages\n",
    "# This cell installs all necessary packages for this lesson\n",
    "# Run this first - it may take a minute!\n",
    "\n",
    "print(\"🚀 Installing packages for Image Recognition Quest...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Install packages using simple pip commands\n",
    "!pip install numpy --quiet\n",
    "!pip install matplotlib --quiet\n",
    "!pip install seaborn --quiet\n",
    "!pip install scikit-learn --quiet\n",
    "!pip install ipywidgets --quiet\n",
    "!pip install tqdm --quiet\n",
    "!pip install pillow --quiet\n",
    "\n",
    "print(\"✅ numpy - Mathematical operations for neural networks\")\n",
    "print(\"✅ matplotlib - Beautiful plots and visualizations\") \n",
    "print(\"✅ seaborn - Enhanced plotting styles\")\n",
    "print(\"✅ scikit-learn - MNIST dataset and utilities\")\n",
    "print(\"✅ ipywidgets - Interactive notebook widgets\")\n",
    "print(\"✅ tqdm - Progress bars for training loops\")\n",
    "print(\"✅ pillow - Image processing capabilities\")\n",
    "\n",
    "print(\"=\" * 60)        \n",
    "print(\"🎉 Setup complete! Ready to build your first vision system!\")\n",
    "print(\"👇 Continue to the next cell to start the quest...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b72d604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 Environment Check & Imports\n",
    "# Let's verify everything is working and import our tools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Set up beautiful plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Enable interactive widgets for Jupyter\n",
    "try:\n",
    "    from IPython.display import display, HTML, clear_output\n",
    "    import ipywidgets as widgets\n",
    "    print(\"✅ Interactive widgets available!\")\n",
    "    WIDGETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"⚠️  Interactive widgets not available (still works fine!)\")\n",
    "    WIDGETS_AVAILABLE = False\n",
    "\n",
    "# Check if we're in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"🌐 Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"💻 Running in local Jupyter\")\n",
    "\n",
    "print(\"🎯 Environment Status:\")\n",
    "print(f\"   Python version: {sys.version.split()[0]}\")\n",
    "print(f\"   NumPy version: {np.__version__}\")\n",
    "print(f\"   Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"   Scikit-learn available: ✅\")\n",
    "print(\"\\n🚀 Ready to start the Image Recognition Quest!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830d954e",
   "metadata": {},
   "source": [
    "# 🖼️ Chapter 1: The Dataset Explorer\n",
    "\n",
    "Before we can teach AI to see, we need to understand what we're working with. Let's meet the famous **MNIST dataset** - 70,000 handwritten digits that have trained countless AI systems!\n",
    "\n",
    "## 🎯 What is MNIST?\n",
    "- **M**odified **N**ational **I**nstitute of **S**tandards and **T**echnology dataset\n",
    "- 28x28 pixel grayscale images of handwritten digits (0-9)\n",
    "- The \"Hello World\" of computer vision\n",
    "- Used by researchers worldwide to test new algorithms\n",
    "\n",
    "Let's load this treasure trove of data and explore it together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd015a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Loading the MNIST Dataset\n",
    "# This is our treasure trove of handwritten digits!\n",
    "\n",
    "print(\"🔍 Loading the famous MNIST dataset...\")\n",
    "print(\"This may take a moment - we're downloading 70,000 images!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load the MNIST dataset\n",
    "# Note: This downloads ~50MB of data the first time\n",
    "start_time = time.time()\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"✅ Dataset loaded in {load_time:.2f} seconds!\")\n",
    "\n",
    "# Extract images and labels\n",
    "X = mnist.data.astype('float32')\n",
    "y = mnist.target.astype('int64')\n",
    "\n",
    "print(\"\\n📊 Dataset Overview:\")\n",
    "print(f\"   Total samples: {X.shape[0]:,}\")\n",
    "print(f\"   Image dimensions: {X.shape[1]} pixels (28x28 flattened)\")\n",
    "print(f\"   Number of classes: {len(np.unique(y))} (digits 0-9)\")\n",
    "print(f\"   Data type: {X.dtype}\")\n",
    "print(f\"   Labels type: {y.dtype}\")\n",
    "\n",
    "# Show memory usage\n",
    "memory_mb = (X.nbytes + y.nbytes) / (1024 * 1024)\n",
    "print(f\"   Memory usage: {memory_mb:.1f} MB\")\n",
    "\n",
    "print(\"\\n🎯 First 10 labels:\", y[:10])\n",
    "print(\"🚀 Dataset ready for exploration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e4233",
   "metadata": {},
   "source": [
    "# 🎨 Chapter 2: Visualizing the Data\n",
    "\n",
    "Let's see what our AI will be learning from! We'll create beautiful visualizations to understand the patterns in handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f322dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎨 Visualizing Sample Digits\n",
    "# Let's see what handwritten digits look like to our AI\n",
    "\n",
    "def visualize_digits(X, y, num_samples=25, title=\"Sample Handwritten Digits\"):\n",
    "    \"\"\"\n",
    "    Create a beautiful grid visualization of handwritten digits\n",
    "    \"\"\"\n",
    "    # Calculate grid dimensions\n",
    "    grid_size = int(np.sqrt(num_samples))\n",
    "    \n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(12, 12))\n",
    "    fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Select random samples\n",
    "    indices = np.random.choice(len(X), num_samples, replace=False)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < num_samples:\n",
    "            # Reshape the flattened image back to 28x28\n",
    "            image = X[indices[i]].reshape(28, 28)\n",
    "            label = y[indices[i]]\n",
    "            \n",
    "            # Display the image\n",
    "            ax.imshow(image, cmap='gray_r', interpolation='nearest')\n",
    "            ax.set_title(f'Label: {label}', fontsize=12, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show a sample of digits\n",
    "print(\"🎨 Let's see what our AI will be learning to recognize:\")\n",
    "visualize_digits(X, y, num_samples=25)\n",
    "\n",
    "# Show the distribution of digits\n",
    "plt.figure(figsize=(12, 6))\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "bars = plt.bar(unique, counts, color=sns.color_palette(\"husl\", 10), alpha=0.8)\n",
    "plt.title('Distribution of Digits in MNIST Dataset', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Digit')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar, count in zip(bars, counts):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 200, \n",
    "             f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 Notice how the dataset is fairly balanced - each digit appears roughly 7,000 times!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861e14f8",
   "metadata": {},
   "source": [
    "# 🔬 Chapter 3: Understanding Image Data\n",
    "\n",
    "Before we feed images to our neural network, we need to understand how computers \"see\" images and prepare our data properly.\n",
    "\n",
    "## 🎯 Key Concepts:\n",
    "- **Pixel Values**: Each pixel has a value from 0 (black) to 255 (white)\n",
    "- **Normalization**: Scaling pixel values to help our neural network learn better\n",
    "- **Flattening**: Converting 2D images to 1D arrays for our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a525a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 Analyzing a Single Digit in Detail\n",
    "# Let's understand how images are represented as numbers\n",
    "\n",
    "def analyze_single_digit(X, y, index=0):\n",
    "    \"\"\"\n",
    "    Detailed analysis of a single digit image\n",
    "    \"\"\"\n",
    "    # Get the image and label\n",
    "    image_flat = X[index]\n",
    "    label = y[index]\n",
    "    image_2d = image_flat.reshape(28, 28)\n",
    "    \n",
    "    print(f\"🔍 Analyzing digit '{label}' at index {index}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create a comprehensive visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle(f'Deep Dive: Handwritten Digit \"{label}\"', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Original image\n",
    "    axes[0, 0].imshow(image_2d, cmap='gray_r', interpolation='nearest')\n",
    "    axes[0, 0].set_title('Original Image (28x28)', fontweight='bold')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Image with pixel values\n",
    "    im = axes[0, 1].imshow(image_2d, cmap='gray_r', interpolation='nearest')\n",
    "    axes[0, 1].set_title('Pixel Values Heatmap', fontweight='bold')\n",
    "    plt.colorbar(im, ax=axes[0, 1], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Histogram of pixel values\n",
    "    axes[0, 2].hist(image_flat, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 2].set_title('Pixel Value Distribution', fontweight='bold')\n",
    "    axes[0, 2].set_xlabel('Pixel Value (0-255)')\n",
    "    axes[0, 2].set_ylabel('Frequency')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Show a zoomed section\n",
    "    zoom_region = image_2d[10:18, 10:18]  # 8x8 section\n",
    "    axes[1, 0].imshow(zoom_region, cmap='gray_r', interpolation='nearest')\n",
    "    axes[1, 0].set_title('Zoomed Section (8x8)', fontweight='bold')\n",
    "    \n",
    "    # Add text annotations for pixel values in zoomed section\n",
    "    for i in range(zoom_region.shape[0]):\n",
    "        for j in range(zoom_region.shape[1]):\n",
    "            axes[1, 0].text(j, i, f'{int(zoom_region[i, j])}', \n",
    "                           ha='center', va='center', fontsize=8, \n",
    "                           color='white' if zoom_region[i, j] < 128 else 'black')\n",
    "    \n",
    "    # 3D surface plot\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    ax_3d = fig.add_subplot(2, 3, 5, projection='3d')\n",
    "    x = np.arange(28)\n",
    "    y = np.arange(28)\n",
    "    X_mesh, Y_mesh = np.meshgrid(x, y)\n",
    "    surface = ax_3d.plot_surface(X_mesh, Y_mesh, image_2d, cmap='viridis', alpha=0.8)\n",
    "    ax_3d.set_title('3D Surface View', fontweight='bold')\n",
    "    ax_3d.set_xlabel('X')\n",
    "    ax_3d.set_ylabel('Y')\n",
    "    ax_3d.set_zlabel('Pixel Value')\n",
    "    \n",
    "    # Statistics\n",
    "    axes[1, 2].axis('off')\n",
    "    stats_text = f\"\"\"\n",
    "📊 Image Statistics:\n",
    "    \n",
    "Shape: {image_2d.shape}\n",
    "Total pixels: {image_2d.size}\n",
    "    \n",
    "Pixel Values:\n",
    "• Min: {image_flat.min():.0f}\n",
    "• Max: {image_flat.max():.0f}\n",
    "• Mean: {image_flat.mean():.1f}\n",
    "• Std: {image_flat.std():.1f}\n",
    "\n",
    "Non-zero pixels: {np.count_nonzero(image_flat)}\n",
    "Zero pixels: {np.sum(image_flat == 0)}\n",
    "\"\"\"\n",
    "    axes[1, 2].text(0.1, 0.5, stats_text, fontsize=12, fontfamily='monospace',\n",
    "                    verticalalignment='center', bbox=dict(boxstyle=\"round,pad=0.3\", \n",
    "                    facecolor=\"lightblue\", alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return image_2d, label\n",
    "\n",
    "# Analyze a few different digits\n",
    "print(\"🔬 Let's dive deep into how images are represented as numbers...\")\n",
    "sample_image, sample_label = analyze_single_digit(X, y, index=0)\n",
    "\n",
    "print(\"\\n🎯 Key Insights:\")\n",
    "print(\"• Each pixel is a number between 0-255\")\n",
    "print(\"• 0 = black (background), 255 = white (ink)\")\n",
    "print(\"• The AI will learn patterns in these numbers\")\n",
    "print(\"• Our job: help the AI focus on the important patterns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c66066",
   "metadata": {},
   "source": [
    "# ⚙️ Chapter 4: Data Preprocessing\n",
    "\n",
    "Now let's prepare our data for the neural network. This is a crucial step that can make or break our AI's performance!\n",
    "\n",
    "## 🎯 What we'll do:\n",
    "1. **Normalize** pixel values (0-1 instead of 0-255)\n",
    "2. **Split** data into training and testing sets\n",
    "3. **Reshape** labels for our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ee7c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚙️ Data Preprocessing Pipeline\n",
    "# Let's prepare our data for optimal AI learning\n",
    "\n",
    "print(\"⚙️ Preparing data for neural network training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Step 1: Normalize pixel values to 0-1 range\n",
    "print(\"🔧 Step 1: Normalizing pixel values...\")\n",
    "X_normalized = X / 255.0  # Convert from 0-255 to 0-1\n",
    "print(f\"   Before: pixels range from {X.min()} to {X.max()}\")\n",
    "print(f\"   After:  pixels range from {X_normalized.min():.3f} to {X_normalized.max():.3f}\")\n",
    "\n",
    "# Step 2: Split into training and testing sets\n",
    "print(\"\\n🔧 Step 2: Splitting data...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_normalized, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"   Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"   Testing set:  {X_test.shape[0]:,} samples\")\n",
    "print(f\"   Training %:   {100 * len(X_train) / len(X):.1f}%\")\n",
    "print(f\"   Testing %:    {100 * len(X_test) / len(X):.1f}%\")\n",
    "\n",
    "# Step 3: One-hot encode labels for neural network\n",
    "print(\"\\n🔧 Step 3: Preparing labels...\")\n",
    "def to_one_hot(labels, num_classes=10):\n",
    "    \"\"\"Convert integer labels to one-hot encoded format\"\"\"\n",
    "    one_hot = np.zeros((len(labels), num_classes))\n",
    "    one_hot[np.arange(len(labels)), labels] = 1\n",
    "    return one_hot\n",
    "\n",
    "y_train_onehot = to_one_hot(y_train)\n",
    "y_test_onehot = to_one_hot(y_test)\n",
    "\n",
    "print(f\"   Original label shape: {y_train.shape}\")\n",
    "print(f\"   One-hot label shape:  {y_train_onehot.shape}\")\n",
    "print(f\"   Example - label {y_train[0]} becomes: {y_train_onehot[0]}\")\n",
    "\n",
    "# Step 4: Data summary\n",
    "print(\"\\n📊 Final Dataset Summary:\")\n",
    "print(f\"   Training images: {X_train.shape}\")\n",
    "print(f\"   Training labels: {y_train_onehot.shape}\")\n",
    "print(f\"   Testing images:  {X_test.shape}\")\n",
    "print(f\"   Testing labels:  {y_test_onehot.shape}\")\n",
    "\n",
    "# Memory usage\n",
    "total_memory = (X_train.nbytes + X_test.nbytes + y_train_onehot.nbytes + y_test_onehot.nbytes) / (1024 * 1024)\n",
    "print(f\"   Total memory:    {total_memory:.1f} MB\")\n",
    "\n",
    "print(\"\\n✅ Data preprocessing complete!\")\n",
    "print(\"🚀 Ready to build our neural network!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b8aac5",
   "metadata": {},
   "source": [
    "# 🧠 Chapter 5: Building the Neural Network\n",
    "\n",
    "Now for the exciting part - building our neural network from scratch! We'll create a multi-layer network that can learn to recognize handwritten digits.\n",
    "\n",
    "## 🏗️ Our Architecture:\n",
    "- **Input Layer**: 784 neurons (28×28 pixels)\n",
    "- **Hidden Layer**: 128 neurons with ReLU activation\n",
    "- **Output Layer**: 10 neurons (one for each digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca4b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧠 Neural Network Implementation\n",
    "# Let's build our image recognition system from scratch!\n",
    "\n",
    "class DigitRecognitionNetwork:\n",
    "    \"\"\"\n",
    "    A neural network for recognizing handwritten digits\n",
    "    Built from scratch with educational clarity in mind!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=784, hidden_size=128, output_size=10, learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        Initialize our neural network\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features (784 for 28x28 images)\n",
    "            hidden_size: Number of neurons in hidden layer\n",
    "            output_size: Number of output classes (10 for digits 0-9)\n",
    "            learning_rate: How fast the network learns\n",
    "        \"\"\"\n",
    "        print(f\"🏗️ Building Neural Network Architecture:\")\n",
    "        print(f\"   Input Layer:  {input_size} neurons (28×28 pixels)\")\n",
    "        print(f\"   Hidden Layer: {hidden_size} neurons (ReLU activation)\")\n",
    "        print(f\"   Output Layer: {output_size} neurons (softmax activation)\")\n",
    "        print(f\"   Learning Rate: {learning_rate}\")\n",
    "        \n",
    "        # Initialize weights with Xavier/Glorot initialization\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Track training history\n",
    "        self.history = {'loss': [], 'accuracy': []}\n",
    "        \n",
    "        print(f\"   Total parameters: {self.count_parameters():,}\")\n",
    "        print(\"✅ Network initialized successfully!\")\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count total number of trainable parameters\"\"\"\n",
    "        return (self.W1.size + self.b1.size + self.W2.size + self.b2.size)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        \"\"\"ReLU activation function\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        \"\"\"Derivative of ReLU function\"\"\"\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"Softmax activation function for output layer\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through the network\n",
    "        \n",
    "        Args:\n",
    "            X: Input data (batch_size, input_size)\n",
    "            \n",
    "        Returns:\n",
    "            predictions: Network output (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        # Hidden layer\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        \n",
    "        # Output layer\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.softmax(self.z2)\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute cross-entropy loss\n",
    "        \n",
    "        Args:\n",
    "            y_true: True labels (one-hot encoded)\n",
    "            y_pred: Predicted probabilities\n",
    "            \n",
    "        Returns:\n",
    "            loss: Average cross-entropy loss\n",
    "        \"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        # Add small epsilon to prevent log(0)\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        loss = -np.sum(y_true * np.log(y_pred)) / m\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, X, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Backward pass (backpropagation)\n",
    "        \n",
    "        Args:\n",
    "            X: Input data\n",
    "            y_true: True labels (one-hot encoded)\n",
    "            y_pred: Predicted probabilities\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dZ2 = y_pred - y_true\n",
    "        dW2 = np.dot(self.a1.T, dZ2) / m\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        dA1 = np.dot(dZ2, self.W2.T)\n",
    "        dZ1 = dA1 * self.relu_derivative(self.z1)\n",
    "        dW1 = np.dot(X.T, dZ1) / m\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "    \n",
    "    def train_batch(self, X, y):\n",
    "        \"\"\"Train on a single batch\"\"\"\n",
    "        # Forward pass\n",
    "        y_pred = self.forward(X)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.compute_loss(y, y_pred)\n",
    "        \n",
    "        # Backward pass\n",
    "        self.backward(X, y, y_pred)\n",
    "        \n",
    "        # Compute accuracy\n",
    "        accuracy = np.mean(np.argmax(y_pred, axis=1) == np.argmax(y, axis=1))\n",
    "        \n",
    "        return loss, accuracy\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        probabilities = self.forward(X)\n",
    "        predictions = np.argmax(probabilities, axis=1)\n",
    "        return predictions, probabilities\n",
    "\n",
    "# Create our neural network\n",
    "print(\"🧠 Creating our Digit Recognition Neural Network...\")\n",
    "network = DigitRecognitionNetwork(\n",
    "    input_size=784, \n",
    "    hidden_size=128, \n",
    "    output_size=10, \n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "print(\"\\n🎯 Network is ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9928107",
   "metadata": {},
   "source": [
    "# 🏃‍♂️ Chapter 6: Training the Network\n",
    "\n",
    "Time to train our AI! We'll watch it learn to recognize digits through multiple epochs, with beautiful visualizations showing its progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734dc310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🏃‍♂️ Training the Neural Network\n",
    "# Watch our AI learn to recognize handwritten digits!\n",
    "\n",
    "def train_network(network, X_train, y_train, X_test, y_test, epochs=50, batch_size=128):\n",
    "    \"\"\"\n",
    "    Train the neural network with beautiful progress tracking\n",
    "    \"\"\"\n",
    "    print(f\"🏃‍♂️ Starting training for {epochs} epochs...\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    print(f\"   Total batches per epoch: {len(X_train) // batch_size}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    # Create progress visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    plt.ion()  # Turn on interactive mode\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = []\n",
    "        epoch_accuracies = []\n",
    "        \n",
    "        # Shuffle training data\n",
    "        indices = np.random.permutation(len(X_train))\n",
    "        X_train_shuffled = X_train[indices]\n",
    "        y_train_shuffled = y_train[indices]\n",
    "        \n",
    "        # Training loop with batches\n",
    "        num_batches = len(X_train) // batch_size\n",
    "        \n",
    "        with tqdm(range(num_batches), desc=f\"Epoch {epoch+1:2d}/{epochs}\") as pbar:\n",
    "            for batch_idx in pbar:\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = start_idx + batch_size\n",
    "                \n",
    "                X_batch = X_train_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_train_shuffled[start_idx:end_idx]\n",
    "                \n",
    "                # Train on batch\n",
    "                loss, accuracy = network.train_batch(X_batch, y_batch)\n",
    "                epoch_losses.append(loss)\n",
    "                epoch_accuracies.append(accuracy)\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\n",
    "                    'Loss': f'{loss:.4f}',\n",
    "                    'Acc': f'{accuracy:.3f}'\n",
    "                })\n",
    "        \n",
    "        # Calculate epoch averages\n",
    "        avg_train_loss = np.mean(epoch_losses)\n",
    "        avg_train_accuracy = np.mean(epoch_accuracies)\n",
    "        \n",
    "        # Test accuracy\n",
    "        test_predictions, _ = network.predict(X_test)\n",
    "        test_accuracy = accuracy_score(np.argmax(y_test, axis=1), test_predictions)\n",
    "        \n",
    "        # Store history\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(avg_train_accuracy)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "        # Update plots every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            # Clear previous plots\n",
    "            ax1.clear()\n",
    "            ax2.clear()\n",
    "            \n",
    "            # Plot loss\n",
    "            ax1.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "            ax1.set_title('Training Loss Over Time', fontweight='bold')\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            ax1.legend()\n",
    "            \n",
    "            # Plot accuracies\n",
    "            ax2.plot(range(1, len(train_accuracies) + 1), train_accuracies, 'g-', label='Training Accuracy', linewidth=2)\n",
    "            ax2.plot(range(1, len(test_accuracies) + 1), test_accuracies, 'r-', label='Test Accuracy', linewidth=2)\n",
    "            ax2.set_title('Accuracy Over Time', fontweight='bold')\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('Accuracy')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            ax2.legend()\n",
    "            ax2.set_ylim(0, 1)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.draw()\n",
    "            plt.pause(0.1)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1:2d}/{epochs} - \"\n",
    "              f\"Loss: {avg_train_loss:.4f} - \"\n",
    "              f\"Train Acc: {avg_train_accuracy:.4f} - \"\n",
    "              f\"Test Acc: {test_accuracy:.4f}\")\n",
    "    \n",
    "    plt.ioff()  # Turn off interactive mode\n",
    "    plt.show()\n",
    "    \n",
    "    return train_losses, train_accuracies, test_accuracies\n",
    "\n",
    "# Start training!\n",
    "print(\"🚀 Let's train our neural network to recognize digits!\")\n",
    "print(\"   This will take a few minutes - watch the magic happen!\")\n",
    "\n",
    "train_losses, train_accs, test_accs = train_network(\n",
    "    network, X_train, y_train_onehot, X_test, y_test_onehot, \n",
    "    epochs=30, batch_size=128\n",
    ")\n",
    "\n",
    "print(\"\\n🎉 Training Complete!\")\n",
    "print(f\"Final Training Accuracy: {train_accs[-1]:.4f}\")\n",
    "print(f\"Final Test Accuracy: {test_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b56554",
   "metadata": {},
   "source": [
    "# 🎯 Chapter 7: Testing Our AI Vision System\n",
    "\n",
    "Let's see how well our AI learned to recognize handwritten digits! We'll test it on new examples and see what it gets right and wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533aaf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Testing Our AI Vision System\n",
    "# Let's see how well our neural network learned!\n",
    "\n",
    "def test_digit_recognition(network, X_test, y_test, num_examples=16):\n",
    "    \"\"\"\n",
    "    Test our neural network with visual examples\n",
    "    \"\"\"\n",
    "    print(\"🎯 Testing our AI's digit recognition abilities...\")\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions, probabilities = network.predict(X_test)\n",
    "    \n",
    "    # Select random test examples\n",
    "    indices = np.random.choice(len(X_test), num_examples, replace=False)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(15, 15))\n",
    "    fig.suptitle('AI Digit Recognition Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < num_examples:\n",
    "            idx = indices[i]\n",
    "            image = X_test[idx].reshape(28, 28)\n",
    "            true_label = np.argmax(y_test[idx])\n",
    "            predicted_label = predictions[idx]\n",
    "            confidence = probabilities[idx][predicted_label]\n",
    "            \n",
    "            # Display image\n",
    "            ax.imshow(image, cmap='gray_r', interpolation='nearest')\n",
    "            \n",
    "            # Color code: green for correct, red for incorrect\n",
    "            color = 'green' if predicted_label == true_label else 'red'\n",
    "            \n",
    "            # Title with prediction info\n",
    "            ax.set_title(f'True: {true_label}, Pred: {predicted_label}\\n'\n",
    "                        f'Confidence: {confidence:.3f}', \n",
    "                        color=color, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    overall_accuracy = accuracy_score(np.argmax(y_test, axis=1), predictions)\n",
    "    print(f\"\\n📊 Overall Test Accuracy: {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)\")\n",
    "    \n",
    "    return predictions, probabilities\n",
    "\n",
    "# Test our network\n",
    "predictions, probabilities = test_digit_recognition(network, X_test, y_test_onehot, num_examples=16)\n",
    "\n",
    "# Show confidence distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Get confidence scores for correct and incorrect predictions\n",
    "correct_mask = predictions == np.argmax(y_test_onehot, axis=1)\n",
    "correct_confidences = np.max(probabilities[correct_mask], axis=1)\n",
    "incorrect_confidences = np.max(probabilities[~correct_mask], axis=1)\n",
    "\n",
    "plt.hist(correct_confidences, bins=30, alpha=0.7, label='Correct Predictions', color='green', density=True)\n",
    "plt.hist(incorrect_confidences, bins=30, alpha=0.7, label='Incorrect Predictions', color='red', density=True)\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Prediction Confidences', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"🎯 Key Insights:\")\n",
    "print(f\"• Correct predictions tend to have higher confidence\")\n",
    "print(f\"• Average confidence for correct: {np.mean(correct_confidences):.3f}\")\n",
    "print(f\"• Average confidence for incorrect: {np.mean(incorrect_confidences):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e742f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Confusion Matrix Analysis\n",
    "# Let's see which digits our AI confuses with each other\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate confusion matrix\n",
    "true_labels = np.argmax(y_test_onehot, axis=1)\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Create beautiful confusion matrix visualization\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=range(10), yticklabels=range(10),\n",
    "            cbar_kws={'label': 'Number of Samples'})\n",
    "plt.title('Confusion Matrix: What Our AI Actually Learned', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Analyze most confused digits\n",
    "print(\"🔍 Most Confused Digit Pairs:\")\n",
    "confusion_pairs = []\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            confusion_pairs.append((i, j, cm[i, j]))\n",
    "\n",
    "# Sort by confusion count\n",
    "confusion_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"   True → Predicted (Count)\")\n",
    "for true_digit, pred_digit, count in confusion_pairs[:10]:\n",
    "    print(f\"   {true_digit} → {pred_digit} ({count} times)\")\n",
    "\n",
    "# Calculate per-digit accuracy\n",
    "print(\"\\n📊 Per-Digit Accuracy:\")\n",
    "for digit in range(10):\n",
    "    digit_mask = true_labels == digit\n",
    "    digit_accuracy = accuracy_score(true_labels[digit_mask], predictions[digit_mask])\n",
    "    print(f\"   Digit {digit}: {digit_accuracy:.3f} ({digit_accuracy*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n🎉 Congratulations! You've built your first image recognition AI system!\")\n",
    "print(\"🎯 Your AI can now recognize handwritten digits with impressive accuracy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f05eb96",
   "metadata": {},
   "source": [
    "# 🎉 Quest Complete: You Built an AI Vision System!\n",
    "\n",
    "## 🏆 **What You've Accomplished**\n",
    "\n",
    "Congratulations! You've just completed one of the most important milestones in AI - building a computer vision system that can recognize handwritten digits. This is the same fundamental technology that powers:\n",
    "\n",
    "- 📱 **Smartphone cameras** that recognize text\n",
    "- 🏦 **Bank check reading** systems  \n",
    "- 📮 **Postal service** address recognition\n",
    "- 🔢 **Captcha systems** on websites\n",
    "\n",
    "## 🧠 **Key Concepts You Mastered**\n",
    "\n",
    "### **Computer Vision Fundamentals**\n",
    "- How computers \"see\" images as numerical arrays\n",
    "- The importance of data preprocessing and normalization\n",
    "- Converting 2D images to 1D vectors for neural networks\n",
    "\n",
    "### **Neural Network Architecture**\n",
    "- Multi-layer perceptron design for classification\n",
    "- ReLU activation for hidden layers\n",
    "- Softmax activation for multi-class output\n",
    "- The role of different layer sizes\n",
    "\n",
    "### **Training Dynamics**\n",
    "- Forward propagation through the network\n",
    "- Backpropagation and gradient descent\n",
    "- Batch training for efficiency\n",
    "- Monitoring loss and accuracy over time\n",
    "\n",
    "### **Model Evaluation**\n",
    "- Test vs. training accuracy\n",
    "- Confusion matrix analysis\n",
    "- Understanding prediction confidence\n",
    "- Identifying common failure modes\n",
    "\n",
    "## 🎯 **Your AI's Performance**\n",
    "\n",
    "Your neural network achieved impressive results:\n",
    "- **Architecture**: 784 → 128 → 10 neurons\n",
    "- **Training accuracy**: ~99%+ \n",
    "- **Test accuracy**: ~97%+\n",
    "- **Total parameters**: ~100,000\n",
    "\n",
    "This performance rivals many production systems!\n",
    "\n",
    "## 🚀 **What's Next?**\n",
    "\n",
    "In our next adventure, **Level 3.2: The Text Understanding Adventure**, we'll tackle an even more exciting challenge - teaching AI to understand human language and emotions in text!\n",
    "\n",
    "### **Preview**: \n",
    "- 📝 Processing text data\n",
    "- 🎭 Sentiment analysis\n",
    "- 🔤 Word embeddings\n",
    "- 🧠 Natural language understanding\n",
    "\n",
    "## 🎖️ **Achievement Unlocked**\n",
    "**🏆 Computer Vision Pioneer**: Successfully built and trained an image recognition system from scratch!\n",
    "\n",
    "---\n",
    "\n",
    "*Keep this notebook as a reference - you've built something truly remarkable! The principles you learned here apply to much more complex vision tasks like face recognition, medical imaging, and autonomous vehicles.*\n",
    "\n",
    "**Ready for the next quest? Let's dive into the fascinating world of AI language understanding!** 🚀"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
