{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7e0a774",
   "metadata": {},
   "source": [
    "# üéÆ Level 4.3: The Reinforcement Learning Odyssey\n",
    "\n",
    "*Can AI learn through trial and error like a human child?*\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **The Ultimate Challenge**\n",
    "\n",
    "Welcome to the final frontier! Today we're building AI that learns by **doing** - no labeled data, no examples, just trial and error. Like teaching a child to ride a bike, our AI will fall, get up, and eventually master complex tasks through pure experience.\n",
    "\n",
    "### **What Makes This \"Rochak\" (Fascinating)?**\n",
    "- üéÆ Watch AI play games and get better over time\n",
    "- üß† See how reward signals shape intelligent behavior\n",
    "- üîÑ Experience the exploration vs exploitation dilemma\n",
    "- üéØ Build AI that discovers winning strategies on its own\n",
    "\n",
    "### **By the End of This Session:**\n",
    "- Build a Q-learning agent from scratch\n",
    "- Train AI to master a simple game\n",
    "- Understand how rewards shape intelligence\n",
    "- Create your own self-learning game AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9104486",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üõ†Ô∏è **Setup: Preparing Our RL Laboratory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c801f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for our RL adventure\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import random\n",
    "from collections import defaultdict, deque\n",
    "import time\n",
    "\n",
    "# Make our plots beautiful\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üéÆ Reinforcement Learning Laboratory Ready!\")\n",
    "print(\"üß† Time to build AI that learns from experience...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a29851d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üåü **The Hook: AI Learning to Play**\n",
    "\n",
    "Let's start with something mind-blowing! Watch an AI agent learn to navigate a maze through pure trial and error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a51d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMaze:\n",
    "    \"\"\"A simple maze environment for our AI to explore\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 0 = empty, 1 = wall, 2 = goal, 3 = agent\n",
    "        self.maze = np.array([\n",
    "            [0, 1, 0, 0, 0],\n",
    "            [0, 1, 0, 1, 0],\n",
    "            [0, 0, 0, 1, 0],\n",
    "            [1, 1, 0, 1, 0],\n",
    "            [0, 0, 0, 0, 2]  # 2 is the goal\n",
    "        ])\n",
    "        self.start_pos = (0, 0)\n",
    "        self.goal_pos = (4, 4)\n",
    "        self.agent_pos = self.start_pos\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset agent to starting position\"\"\"\n",
    "        self.agent_pos = self.start_pos\n",
    "        return self.agent_pos\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action and return new state, reward, done\"\"\"\n",
    "        # Actions: 0=up, 1=right, 2=down, 3=left\n",
    "        moves = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n",
    "        \n",
    "        old_pos = self.agent_pos\n",
    "        new_row = old_pos[0] + moves[action][0]\n",
    "        new_col = old_pos[1] + moves[action][1]\n",
    "        \n",
    "        # Check boundaries and walls\n",
    "        if (0 <= new_row < 5 and 0 <= new_col < 5 and \n",
    "            self.maze[new_row, new_col] != 1):\n",
    "            self.agent_pos = (new_row, new_col)\n",
    "        \n",
    "        # Calculate reward\n",
    "        if self.agent_pos == self.goal_pos:\n",
    "            reward = 100  # Big reward for reaching goal!\n",
    "            done = True\n",
    "        elif self.agent_pos == old_pos:  # Hit wall\n",
    "            reward = -10  # Penalty for hitting wall\n",
    "            done = False\n",
    "        else:\n",
    "            reward = -1  # Small penalty for each step\n",
    "            done = False\n",
    "            \n",
    "        return self.agent_pos, reward, done\n",
    "    \n",
    "    def visualize(self):\n",
    "        \"\"\"Show the current state of the maze\"\"\"\n",
    "        visual = self.maze.copy()\n",
    "        visual[self.agent_pos] = 3  # Mark agent position\n",
    "        \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(visual, cmap='viridis')\n",
    "        plt.title('üéÆ Maze Environment\\nüü¶=Empty üü´=Wall üü®=Goal üü™=Agent')\n",
    "        \n",
    "        # Add grid lines\n",
    "        for i in range(6):\n",
    "            plt.axhline(i-0.5, color='white', linewidth=2)\n",
    "            plt.axvline(i-0.5, color='white', linewidth=2)\n",
    "            \n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.show()\n",
    "\n",
    "# Create and visualize our maze\n",
    "maze = SimpleMaze()\n",
    "maze.visualize()\n",
    "print(\"üéØ Goal: Train an AI agent to find the shortest path to the yellow goal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c9e31c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† **The Science: What is Reinforcement Learning?**\n",
    "\n",
    "Reinforcement Learning is how we teach AI to make sequences of decisions by learning from rewards and punishments - just like training a pet or learning to drive!\n",
    "\n",
    "### **Key Concepts:**\n",
    "- **Agent**: The AI making decisions (our maze navigator)\n",
    "- **Environment**: The world the agent interacts with (the maze)\n",
    "- **State**: Current situation (agent's position)\n",
    "- **Action**: What the agent can do (move up/down/left/right)\n",
    "- **Reward**: Feedback for actions (positive for goal, negative for walls)\n",
    "- **Policy**: Strategy for choosing actions\n",
    "\n",
    "### **The Learning Loop:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769dabd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the RL learning loop\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Create a circular diagram\n",
    "theta = np.linspace(0, 2*np.pi, 5)\n",
    "radius = 3\n",
    "x = radius * np.cos(theta)\n",
    "y = radius * np.sin(theta)\n",
    "\n",
    "# Components\n",
    "components = ['ü§ñ\\nAgent', 'üåç\\nEnvironment', 'üìç\\nState', 'üé¨\\nAction', 'üéÅ\\nReward']\n",
    "colors = ['lightblue', 'lightgreen', 'lightyellow', 'lightcoral', 'lightpink']\n",
    "\n",
    "# Plot components\n",
    "for i, (xi, yi, comp, color) in enumerate(zip(x, y, components, colors)):\n",
    "    circle = plt.Circle((xi, yi), 0.8, color=color, alpha=0.7)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(xi, yi, comp, ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add arrows to show the flow\n",
    "arrow_props = dict(arrowstyle='->', lw=3, color='darkblue')\n",
    "ax.annotate('', xy=(x[1], y[1]), xytext=(x[0], y[0]), arrowprops=arrow_props)\n",
    "ax.annotate('', xy=(x[2], y[2]), xytext=(x[1], y[1]), arrowprops=arrow_props)\n",
    "ax.annotate('', xy=(x[3], y[3]), xytext=(x[2], y[2]), arrowprops=arrow_props)\n",
    "ax.annotate('', xy=(x[4], y[4]), xytext=(x[3], y[3]), arrowprops=arrow_props)\n",
    "ax.annotate('', xy=(x[0], y[0]), xytext=(x[4], y[4]), arrowprops=arrow_props)\n",
    "\n",
    "ax.set_xlim(-5, 5)\n",
    "ax.set_ylim(-5, 5)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('üîÑ The Reinforcement Learning Loop', fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üß† This is how AI learns from experience - one step at a time!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675fe822",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ **The Challenge: Building Q-Learning from Scratch**\n",
    "\n",
    "We'll build a **Q-learning** agent that learns the value of taking each action in each state. Think of it as building a \"cheat sheet\" for the perfect strategy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73910775",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"A Q-learning agent that learns through trial and error\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.95, \n",
    "                 exploration_rate=1.0, exploration_decay=0.995):\n",
    "        # The Q-table: Q(state, action) = expected future reward\n",
    "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.lr = learning_rate          # How fast we learn\n",
    "        self.gamma = discount_factor     # How much we care about future rewards\n",
    "        self.epsilon = exploration_rate  # How much we explore vs exploit\n",
    "        self.epsilon_decay = exploration_decay\n",
    "        \n",
    "        # For tracking learning progress\n",
    "        self.episode_rewards = []\n",
    "        self.episode_steps = []\n",
    "    \n",
    "    def choose_action(self, state, available_actions=[0, 1, 2, 3]):\n",
    "        \"\"\"Choose action using epsilon-greedy strategy\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            # Explore: choose random action\n",
    "            return random.choice(available_actions)\n",
    "        else:\n",
    "            # Exploit: choose best known action\n",
    "            q_values = [self.q_table[state][action] for action in available_actions]\n",
    "            max_q = max(q_values)\n",
    "            # If multiple actions have same max Q, choose randomly among them\n",
    "            best_actions = [action for action, q in zip(available_actions, q_values) if q == max_q]\n",
    "            return random.choice(best_actions)\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Update Q-table using the Q-learning formula\"\"\"\n",
    "        current_q = self.q_table[state][action]\n",
    "        \n",
    "        if done:\n",
    "            # No future rewards if episode is done\n",
    "            target_q = reward\n",
    "        else:\n",
    "            # Q-learning formula: Q(s,a) = r + Œ≥ * max(Q(s',a'))\n",
    "            next_max_q = max([self.q_table[next_state][a] for a in [0, 1, 2, 3]])\n",
    "            target_q = reward + self.gamma * next_max_q\n",
    "        \n",
    "        # Update Q-value\n",
    "        self.q_table[state][action] += self.lr * (target_q - current_q)\n",
    "        \n",
    "        # Decay exploration rate\n",
    "        if done:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            self.epsilon = max(0.01, self.epsilon)  # Minimum exploration\n",
    "    \n",
    "    def get_policy(self):\n",
    "        \"\"\"Extract the learned policy from Q-table\"\"\"\n",
    "        policy = {}\n",
    "        for state in self.q_table:\n",
    "            q_values = [self.q_table[state][action] for action in [0, 1, 2, 3]]\n",
    "            policy[state] = np.argmax(q_values)\n",
    "        return policy\n",
    "\n",
    "print(\"ü§ñ Q-Learning Agent Created!\")\n",
    "print(\"üß† Ready to learn through trial and error...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d9c9ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî® **The Build: Training Our AI Agent**\n",
    "\n",
    "Now let's train our agent to master the maze! Watch it go from random wandering to optimal navigation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a8ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(episodes=1000, visualize_every=100):\n",
    "    \"\"\"Train the Q-learning agent\"\"\"\n",
    "    agent = QLearningAgent()\n",
    "    environment = SimpleMaze()\n",
    "    \n",
    "    print(f\"üéØ Training agent for {episodes} episodes...\")\n",
    "    print(\"üìä Watch the learning progress!\")\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = environment.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        max_steps = 100  # Prevent infinite loops\n",
    "        \n",
    "        while steps < max_steps:\n",
    "            # Agent chooses action\n",
    "            action = agent.choose_action(state)\n",
    "            \n",
    "            # Environment responds\n",
    "            next_state, reward, done = environment.step(action)\n",
    "            \n",
    "            # Agent learns from experience\n",
    "            agent.learn(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Update state and stats\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Track progress\n",
    "        agent.episode_rewards.append(total_reward)\n",
    "        agent.episode_steps.append(steps)\n",
    "        \n",
    "        # Show progress\n",
    "        if (episode + 1) % visualize_every == 0:\n",
    "            avg_reward = np.mean(agent.episode_rewards[-visualize_every:])\n",
    "            avg_steps = np.mean(agent.episode_steps[-visualize_every:])\n",
    "            print(f\"Episode {episode+1}: Avg Reward = {avg_reward:.1f}, Avg Steps = {avg_steps:.1f}, Exploration = {agent.epsilon:.3f}\")\n",
    "    \n",
    "    return agent\n",
    "\n",
    "# Train our agent!\n",
    "trained_agent = train_agent(episodes=1000)\n",
    "print(\"\\nüéâ Training Complete!\")\n",
    "print(\"üß† Agent has learned to navigate the maze!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f34aafd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä **The Visualization: Watching AI Learn**\n",
    "\n",
    "Let's visualize how our AI's performance improved over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d7dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_learning_progress(agent):\n",
    "    \"\"\"Show how the agent's performance improved during training\"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Rewards over time\n",
    "    episodes = range(len(agent.episode_rewards))\n",
    "    ax1.plot(episodes, agent.episode_rewards, alpha=0.3, color='blue')\n",
    "    \n",
    "    # Moving average for clearer trend\n",
    "    window = 50\n",
    "    if len(agent.episode_rewards) >= window:\n",
    "        moving_avg = np.convolve(agent.episode_rewards, np.ones(window)/window, mode='valid')\n",
    "        ax1.plot(range(window-1, len(agent.episode_rewards)), moving_avg, \n",
    "                color='red', linewidth=2, label=f'Moving Average ({window} episodes)')\n",
    "        ax1.legend()\n",
    "    \n",
    "    ax1.set_title('üéÅ Episode Rewards Over Time')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Total Reward')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # 2. Steps to goal over time\n",
    "    ax2.plot(episodes, agent.episode_steps, alpha=0.3, color='green')\n",
    "    if len(agent.episode_steps) >= window:\n",
    "        moving_avg_steps = np.convolve(agent.episode_steps, np.ones(window)/window, mode='valid')\n",
    "        ax2.plot(range(window-1, len(agent.episode_steps)), moving_avg_steps,\n",
    "                color='darkgreen', linewidth=2, label=f'Moving Average ({window} episodes)')\n",
    "        ax2.legend()\n",
    "    \n",
    "    ax2.set_title('üë£ Steps to Reach Goal')\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Steps')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # 3. Q-table heatmap\n",
    "    q_values_grid = np.zeros((5, 5))\n",
    "    for (row, col) in trained_agent.q_table:\n",
    "        if 0 <= row < 5 and 0 <= col < 5:\n",
    "            max_q = max([trained_agent.q_table[(row, col)][action] for action in [0, 1, 2, 3]])\n",
    "            q_values_grid[row, col] = max_q\n",
    "    \n",
    "    im = ax3.imshow(q_values_grid, cmap='viridis')\n",
    "    ax3.set_title('üß† Learned State Values (Q-table)')\n",
    "    ax3.set_xlabel('Column')\n",
    "    ax3.set_ylabel('Row')\n",
    "    plt.colorbar(im, ax=ax3, label='Max Q-value')\n",
    "    \n",
    "    # 4. Learned policy visualization\n",
    "    policy = trained_agent.get_policy()\n",
    "    policy_grid = np.full((5, 5), -1)  # -1 for walls/no policy\n",
    "    \n",
    "    maze_layout = np.array([\n",
    "        [0, 1, 0, 0, 0],\n",
    "        [0, 1, 0, 1, 0],\n",
    "        [0, 0, 0, 1, 0],\n",
    "        [1, 1, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 2]\n",
    "    ])\n",
    "    \n",
    "    for (row, col) in policy:\n",
    "        if 0 <= row < 5 and 0 <= col < 5 and maze_layout[row, col] != 1:\n",
    "            policy_grid[row, col] = policy[(row, col)]\n",
    "    \n",
    "    ax4.imshow(maze_layout, cmap='gray', alpha=0.3)\n",
    "    \n",
    "    # Add arrows for policy\n",
    "    arrows = ['‚Üë', '‚Üí', '‚Üì', '‚Üê']\n",
    "    for row in range(5):\n",
    "        for col in range(5):\n",
    "            if policy_grid[row, col] != -1:\n",
    "                arrow = arrows[int(policy_grid[row, col])]\n",
    "                ax4.text(col, row, arrow, ha='center', va='center', \n",
    "                        fontsize=20, color='red', fontweight='bold')\n",
    "    \n",
    "    ax4.set_title('üó∫Ô∏è Learned Policy (Optimal Actions)')\n",
    "    ax4.set_xlabel('Column')\n",
    "    ax4.set_ylabel('Row')\n",
    "    ax4.set_xticks(range(5))\n",
    "    ax4.set_yticks(range(5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the learning progress\n",
    "visualize_learning_progress(trained_agent)\n",
    "print(\"üìä Amazing! You can see how the AI learned to solve the maze efficiently!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e2f03a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéÆ **The Demo: Watch the Trained Agent in Action**\n",
    "\n",
    "Let's see our trained agent navigate the maze optimally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39b8f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_trained_agent(agent, num_demos=3):\n",
    "    \"\"\"Show the trained agent solving the maze\"\"\"\n",
    "    environment = SimpleMaze()\n",
    "    \n",
    "    print(f\"üé≠ Demonstrating trained agent ({num_demos} runs)\")\n",
    "    print(\"üéØ Watch how it takes the optimal path!\\n\")\n",
    "    \n",
    "    for demo in range(num_demos):\n",
    "        print(f\"üéÆ Demo {demo + 1}:\")\n",
    "        state = environment.reset()\n",
    "        path = [state]\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        # Use trained policy (no exploration)\n",
    "        old_epsilon = agent.epsilon\n",
    "        agent.epsilon = 0  # Pure exploitation\n",
    "        \n",
    "        while steps < 20:  # Safety limit\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = environment.step(action)\n",
    "            \n",
    "            path.append(next_state)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            action_names = ['UP', 'RIGHT', 'DOWN', 'LEFT']\n",
    "            print(f\"  Step {steps}: {state} --{action_names[action]}--> {next_state} (reward: {reward})\")\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                print(f\"  üéâ Goal reached in {steps} steps! Total reward: {total_reward}\")\n",
    "                break\n",
    "        \n",
    "        # Restore exploration rate\n",
    "        agent.epsilon = old_epsilon\n",
    "        print(f\"  üìç Path taken: {' -> '.join(map(str, path))}\\n\")\n",
    "\n",
    "# Demonstrate our trained agent\n",
    "demonstrate_trained_agent(trained_agent)\n",
    "print(\"üß† The agent has learned the optimal strategy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234d71e6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí° **The Aha Moment: Understanding Reinforcement Learning**\n",
    "\n",
    "### **What Just Happened?**\n",
    "\n",
    "1. **Trial and Error Learning**: Our AI started with no knowledge and learned purely through experience\n",
    "\n",
    "2. **Value Function Discovery**: The Q-table learned the \"value\" of being in each state and taking each action\n",
    "\n",
    "3. **Exploration vs Exploitation**: The agent balanced trying new things (exploration) with using what it learned (exploitation)\n",
    "\n",
    "4. **Emergent Intelligence**: Complex behavior (optimal navigation) emerged from simple rules (Q-learning)\n",
    "\n",
    "### **The Deep Insight:**\n",
    "**Intelligence isn't programmed - it's discovered through interaction with the environment!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab43b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore what the agent actually learned\n",
    "def analyze_learned_knowledge(agent):\n",
    "    \"\"\"Analyze what the agent learned about the environment\"\"\"\n",
    "    print(\"üîç Analyzing what our AI learned...\\n\")\n",
    "    \n",
    "    # Show Q-values for some interesting states\n",
    "    interesting_states = [(0, 0), (2, 2), (4, 3), (4, 4)]\n",
    "    action_names = ['UP', 'RIGHT', 'DOWN', 'LEFT']\n",
    "    \n",
    "    for state in interesting_states:\n",
    "        if state in agent.q_table:\n",
    "            print(f\"üìç State {state}:\")\n",
    "            q_values = [agent.q_table[state][action] for action in [0, 1, 2, 3]]\n",
    "            best_action = np.argmax(q_values)\n",
    "            \n",
    "            for action, q_val in enumerate(q_values):\n",
    "                marker = \"üåü\" if action == best_action else \"  \"\n",
    "                print(f\"  {marker} {action_names[action]}: {q_val:.2f}\")\n",
    "            print(f\"  ‚û°Ô∏è Best action: {action_names[best_action]}\\n\")\n",
    "    \n",
    "    # Show learning statistics\n",
    "    print(\"üìä Learning Statistics:\")\n",
    "    print(f\"  üéØ Final exploration rate: {agent.epsilon:.3f}\")\n",
    "    print(f\"  üìà Average reward (last 100 episodes): {np.mean(agent.episode_rewards[-100:]):.1f}\")\n",
    "    print(f\"  üë£ Average steps (last 100 episodes): {np.mean(agent.episode_steps[-100:]):.1f}\")\n",
    "    print(f\"  üß† Total states learned: {len(agent.q_table)}\")\n",
    "\n",
    "analyze_learned_knowledge(trained_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96dd65b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß™ **The Practice: Experiment and Explore**\n",
    "\n",
    "Now it's your turn to experiment! Try modifying different aspects and see how it affects learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a57b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Different learning rates\n",
    "def experiment_learning_rates():\n",
    "    \"\"\"Compare agents with different learning rates\"\"\"\n",
    "    learning_rates = [0.01, 0.1, 0.5, 0.9]\n",
    "    results = {}\n",
    "    \n",
    "    print(\"üß™ Experiment: How does learning rate affect performance?\")\n",
    "    print(\"üìä Training agents with different learning rates...\\n\")\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        print(f\"Training with learning rate = {lr}...\")\n",
    "        agent = QLearningAgent(learning_rate=lr)\n",
    "        environment = SimpleMaze()\n",
    "        \n",
    "        # Quick training\n",
    "        for episode in range(300):\n",
    "            state = environment.reset()\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            \n",
    "            while steps < 50:\n",
    "                action = agent.choose_action(state)\n",
    "                next_state, reward, done = environment.step(action)\n",
    "                agent.learn(state, action, reward, next_state, done)\n",
    "                \n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            agent.episode_rewards.append(total_reward)\n",
    "        \n",
    "        # Store results\n",
    "        avg_final_reward = np.mean(agent.episode_rewards[-50:])\n",
    "        results[lr] = avg_final_reward\n",
    "        print(f\"  Final average reward: {avg_final_reward:.1f}\")\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    lrs = list(results.keys())\n",
    "    rewards = list(results.values())\n",
    "    \n",
    "    plt.bar(lrs, rewards, color='skyblue', alpha=0.7)\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('Average Final Reward')\n",
    "    plt.title('üß™ Effect of Learning Rate on Performance')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for lr, reward in zip(lrs, rewards):\n",
    "        plt.text(lr, reward + 1, f'{reward:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"\\nüí° Insight: Moderate learning rates often work best!\")\n",
    "\n",
    "# Run the experiment\n",
    "experiment_learning_rates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b57b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Create your own maze!\n",
    "def create_custom_maze():\n",
    "    \"\"\"Create and test your own maze design\"\"\"\n",
    "    print(\"üé® Design Your Own Maze!\")\n",
    "    print(\"üìù Modify the maze layout below and see how the agent learns:\")\n",
    "    \n",
    "    class CustomMaze(SimpleMaze):\n",
    "        def __init__(self):\n",
    "            # Design your maze here! 0=empty, 1=wall, 2=goal\n",
    "            # Try making it more complex!\n",
    "            self.maze = np.array([\n",
    "                [0, 0, 1, 0, 0],\n",
    "                [1, 0, 1, 0, 1],\n",
    "                [0, 0, 0, 0, 0],\n",
    "                [0, 1, 1, 1, 0],\n",
    "                [0, 0, 0, 0, 2]\n",
    "            ])\n",
    "            self.start_pos = (0, 0)\n",
    "            self.goal_pos = (4, 4)\n",
    "            self.agent_pos = self.start_pos\n",
    "    \n",
    "    # Train agent on custom maze\n",
    "    custom_agent = QLearningAgent()\n",
    "    custom_env = CustomMaze()\n",
    "    \n",
    "    print(\"üó∫Ô∏è Your custom maze:\")\n",
    "    custom_env.visualize()\n",
    "    \n",
    "    print(\"üéØ Training agent on your maze...\")\n",
    "    for episode in range(500):\n",
    "        state = custom_env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while steps < 100:\n",
    "            action = custom_agent.choose_action(state)\n",
    "            next_state, reward, done = custom_env.step(action)\n",
    "            custom_agent.learn(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        custom_agent.episode_rewards.append(total_reward)\n",
    "        custom_agent.episode_steps.append(steps)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Training complete!\")\n",
    "    print(f\"üìä Final performance: {np.mean(custom_agent.episode_rewards[-50:]):.1f} average reward\")\n",
    "    \n",
    "    # Show learned policy\n",
    "    policy = custom_agent.get_policy()\n",
    "    print(\"\\nüß† Learned policy for your maze:\")\n",
    "    \n",
    "    return custom_agent\n",
    "\n",
    "# Try creating your own maze!\n",
    "# Uncomment the line below to run\n",
    "# my_agent = create_custom_maze()\n",
    "\n",
    "print(\"üé® Uncomment the line above to design your own maze!\")\n",
    "print(\"üí° Try different layouts and see how it affects learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4247f18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üåâ **The Bridge: What's Next in Your RL Journey?**\n",
    "\n",
    "### **üéä What You've Accomplished:**\n",
    "- Built a complete Q-learning agent from scratch\n",
    "- Trained AI to solve a navigation problem through pure trial and error\n",
    "- Understood the exploration vs exploitation trade-off\n",
    "- Visualized how AI discovers optimal strategies\n",
    "\n",
    "### **üöÄ Where Reinforcement Learning Goes Next:**\n",
    "\n",
    "1. **üéÆ Game AI**: Train agents to master Atari games, Chess, Go\n",
    "2. **ü§ñ Robotics**: Teach robots to walk, manipulate objects, navigate\n",
    "3. **üè≠ Industrial Control**: Optimize manufacturing, energy systems\n",
    "4. **üí∞ Finance**: Algorithmic trading, portfolio optimization\n",
    "5. **üöó Autonomous Vehicles**: Decision making in complex environments\n",
    "\n",
    "### **üî¨ Advanced RL Techniques to Explore:**\n",
    "- **Deep Q-Networks (DQN)**: Q-learning with neural networks\n",
    "- **Policy Gradient Methods**: Learn policies directly\n",
    "- **Actor-Critic Methods**: Combine value and policy learning\n",
    "- **Multi-Agent RL**: Multiple agents learning together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview: Deep Q-Learning concept\n",
    "def preview_deep_rl():\n",
    "    \"\"\"Show the concept of Deep Reinforcement Learning\"\"\"\n",
    "    print(\"üîÆ Preview: Deep Reinforcement Learning\")\n",
    "    print(\"üß† Instead of a Q-table, use a neural network!\")\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Traditional Q-learning\n",
    "    ax1.text(0.5, 0.8, 'üìä Q-Table', ha='center', va='center', \n",
    "             fontsize=16, transform=ax1.transAxes, fontweight='bold')\n",
    "    ax1.text(0.5, 0.6, 'State ‚Üí Look up ‚Üí Action', ha='center', va='center',\n",
    "             fontsize=12, transform=ax1.transAxes)\n",
    "    ax1.text(0.5, 0.4, '‚úÖ Simple, interpretable', ha='center', va='center',\n",
    "             fontsize=11, transform=ax1.transAxes, color='green')\n",
    "    ax1.text(0.5, 0.3, '‚ùå Limited to small state spaces', ha='center', va='center',\n",
    "             fontsize=11, transform=ax1.transAxes, color='red')\n",
    "    ax1.set_title('Traditional Q-Learning')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Deep Q-learning\n",
    "    ax2.text(0.5, 0.8, 'üß† Neural Network', ha='center', va='center',\n",
    "             fontsize=16, transform=ax2.transAxes, fontweight='bold')\n",
    "    ax2.text(0.5, 0.6, 'State ‚Üí Neural Net ‚Üí Q-values', ha='center', va='center',\n",
    "             fontsize=12, transform=ax2.transAxes)\n",
    "    ax2.text(0.5, 0.4, '‚úÖ Handles complex states (images, etc.)', ha='center', va='center',\n",
    "             fontsize=11, transform=ax2.transAxes, color='green')\n",
    "    ax2.text(0.5, 0.3, '‚úÖ Can generalize to unseen states', ha='center', va='center',\n",
    "             fontsize=11, transform=ax2.transAxes, color='green')\n",
    "    ax2.set_title('Deep Q-Learning (DQN)')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüéÆ Famous Example: DeepMind's DQN learned to play Atari games from pixels!\")\n",
    "    print(\"üöÄ This opened the door to modern AI achievements like AlphaGo!\")\n",
    "\n",
    "preview_deep_rl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416f8e09",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ **Final Challenge: Your RL Portfolio Project**\n",
    "\n",
    "Ready to showcase your reinforcement learning skills? Here are some project ideas:\n",
    "\n",
    "### **üéÆ Beginner Projects:**\n",
    "1. **Tic-Tac-Toe Master**: Train an agent to play optimal tic-tac-toe\n",
    "2. **Grid World Explorer**: Create different maze layouts and compare learning\n",
    "3. **Simple Trading Bot**: Buy/sell decisions based on price patterns\n",
    "\n",
    "### **üöÄ Intermediate Projects:**\n",
    "1. **Snake Game AI**: Train an agent to play the classic Snake game\n",
    "2. **Portfolio Manager**: Multi-asset investment decisions\n",
    "3. **Traffic Light Controller**: Optimize traffic flow at intersections\n",
    "\n",
    "### **üåü Advanced Projects:**\n",
    "1. **Multiplayer Game AI**: Agents that adapt to human strategies\n",
    "2. **Resource Management**: Optimize server allocation or energy distribution\n",
    "3. **Creative AI**: Generate music or art through reward-based learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congratulations message\n",
    "def celebration():\n",
    "    \"\"\"Celebrate completing the Reinforcement Learning journey!\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ CONGRATULATIONS! üéâ\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"üß† You've mastered Reinforcement Learning!\")\n",
    "    print(\"üéÆ You can now build AI that learns from experience!\")\n",
    "    print(\"üöÄ You understand how intelligence emerges from interaction!\")\n",
    "    print(\"\")\n",
    "    print(\"üåü What you've accomplished:\")\n",
    "    print(\"   ‚úÖ Built Q-learning from mathematical foundations\")\n",
    "    print(\"   ‚úÖ Trained AI agents through trial and error\")\n",
    "    print(\"   ‚úÖ Understood exploration vs exploitation\")\n",
    "    print(\"   ‚úÖ Visualized the learning process\")\n",
    "    print(\"   ‚úÖ Experimented with different parameters\")\n",
    "    print(\"\")\n",
    "    print(\"üéØ You're now ready to:\")\n",
    "    print(\"   üéÆ Build game-playing AI\")\n",
    "    print(\"   ü§ñ Train autonomous agents\")\n",
    "    print(\"   üè≠ Optimize real-world systems\")\n",
    "    print(\"   üî¨ Explore advanced RL techniques\")\n",
    "    print(\"\")\n",
    "    print(\"üí´ Keep exploring, keep building, keep learning!\")\n",
    "    print(\"üåü The future of AI is in your hands!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "celebration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e001b0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö **Summary: Your Reinforcement Learning Mastery**\n",
    "\n",
    "### **üéØ Core Concepts Mastered:**\n",
    "- **Agent-Environment Interaction**: How AI learns through experience\n",
    "- **Q-Learning Algorithm**: Value-based learning from rewards\n",
    "- **Exploration vs Exploitation**: Balancing discovery and optimization\n",
    "- **Policy Learning**: Discovering optimal action strategies\n",
    "\n",
    "### **üõ†Ô∏è Technical Skills Gained:**\n",
    "- Implemented Q-learning from scratch\n",
    "- Built custom environments for training\n",
    "- Visualized learning progress and policies\n",
    "- Experimented with hyperparameter tuning\n",
    "\n",
    "### **üí° Key Insights:**\n",
    "1. **Intelligence emerges from interaction** - no pre-programming needed\n",
    "2. **Rewards shape behavior** - proper reward design is crucial\n",
    "3. **Learning is gradual** - patience and iteration lead to mastery\n",
    "4. **Generalization is powerful** - learned strategies work in new situations\n",
    "\n",
    "---\n",
    "\n",
    "*\"The best way to predict the future is to create it. You've just learned how to create intelligent behavior from nothing but experience and rewards. That's the essence of learning itself!\"*\n",
    "\n",
    "**üöÄ Ready for your next AI adventure? The possibilities are infinite!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
