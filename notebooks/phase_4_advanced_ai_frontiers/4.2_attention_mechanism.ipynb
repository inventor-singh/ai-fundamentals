{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca7979f2",
   "metadata": {},
   "source": [
    "# üîç Level 4.2: The Attention Mechanism\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/ai-mastery-from-scratch/blob/main/notebooks/phase_4_advanced_ai_frontiers/4.2_attention_mechanism.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **The Challenge**\n",
    "**How does AI focus on the most important parts of data?**\n",
    "\n",
    "Welcome to the revolutionary world of Attention! This is the breakthrough that enabled modern AI systems like ChatGPT, GPT-4, and BERT. Today we'll build the attention mechanism from scratch and see how it allows AI to selectively focus on relevant information, just like human attention works.\n",
    "\n",
    "### **What You'll Discover:**\n",
    "- üîç How attention mimics human selective focus\n",
    "- üß† The mathematics behind self-attention\n",
    "- üéØ Query, Key, Value - the trinity of attention\n",
    "- ‚ú® Multi-head attention for parallel processing\n",
    "\n",
    "### **What You'll Build:**\n",
    "A complete attention mechanism that can focus on important words in sentences and relevant parts of sequences!\n",
    "\n",
    "### **The Journey Ahead:**\n",
    "1. **The Focus Foundation** - Understanding attention intuitively\n",
    "2. **The QKV Trinity** - Query, Key, Value mechanics\n",
    "3. **The Attention Computer** - Building scaled dot-product attention\n",
    "4. **The Multi-Head Processor** - Parallel attention computation\n",
    "5. **The Transformer Engine** - Putting it all together\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ **Setup & Installation**\n",
    "\n",
    "*Run the cells below to set up your environment. This works in both Google Colab and local Jupyter notebooks.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916104ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Install Required Packages\n",
    "# This cell installs all necessary packages for this lesson\n",
    "# Run this first - it may take a minute!\n",
    "\n",
    "print(\"üöÄ Installing packages for Attention Mechanism...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Install packages using simple pip commands\n",
    "!pip install numpy --quiet\n",
    "!pip install matplotlib --quiet\n",
    "!pip install seaborn --quiet\n",
    "!pip install ipywidgets --quiet\n",
    "!pip install tqdm --quiet\n",
    "\n",
    "print(\"‚úÖ numpy - Mathematical operations for neural networks\")\n",
    "print(\"‚úÖ matplotlib - Beautiful plots and visualizations\") \n",
    "print(\"‚úÖ seaborn - Enhanced plotting styles and heatmaps\")\n",
    "print(\"‚úÖ ipywidgets - Interactive notebook widgets\")\n",
    "print(\"‚úÖ tqdm - Progress bars for training loops\")\n",
    "\n",
    "print(\"=\" * 60)        \n",
    "print(\"üéâ Setup complete! Ready to build attention mechanisms!\")\n",
    "print(\"üëá Continue to the next cell to start focusing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0dd55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Environment Check & Imports\n",
    "# Let's verify everything is working and import our tools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Set up beautiful plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Enable interactive widgets for Jupyter\n",
    "try:\n",
    "    from IPython.display import display, HTML, clear_output\n",
    "    import ipywidgets as widgets\n",
    "    print(\"‚úÖ Interactive widgets available!\")\n",
    "    WIDGETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Interactive widgets not available (still works fine!)\")\n",
    "    WIDGETS_AVAILABLE = False\n",
    "\n",
    "# Check if we're in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"üåê Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"üíª Running in local Jupyter\")\n",
    "\n",
    "print(\"üéØ Environment Status:\")\n",
    "print(f\"   Python version: {sys.version.split()[0]}\")\n",
    "print(f\"   NumPy version: {np.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\\nüöÄ Ready to build attention mechanisms!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f545a59e",
   "metadata": {},
   "source": [
    "# üß† Chapter 1: Understanding Attention Intuitively\n",
    "\n",
    "Before diving into the mathematics, let's understand what attention means and why it's revolutionary. Attention allows AI to focus on relevant parts of input data, just like how humans focus on important words in a sentence.\n",
    "\n",
    "## üéØ Real-World Attention Examples:\n",
    "\n",
    "### **Reading a Sentence**:\n",
    "*\"The cat sat on the **mat** while the dog played with the **ball**\"*\n",
    "- When asked \"Where did the cat sit?\", we **attend** to \"mat\"\n",
    "- When asked \"What did the dog play with?\", we **attend** to \"ball\"\n",
    "\n",
    "### **Looking at an Image**:\n",
    "- When asked \"What color is the car?\", we **attend** to the car\n",
    "- When asked \"What's in the sky?\", we **attend** to clouds/birds\n",
    "\n",
    "Let's start by building a simple attention mechanism with text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d819962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Simple Text Processing Setup\n",
    "# Let's create some sample text to work with attention\n",
    "\n",
    "# Create a simple vocabulary and text processing system\n",
    "class SimpleTextProcessor:\n",
    "    \"\"\"\n",
    "    A simple text processor for demonstrating attention\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the text processor\"\"\"\n",
    "        # Simple vocabulary for demonstration\n",
    "        self.vocab = {\n",
    "            '<PAD>': 0, 'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5,\n",
    "            'dog': 6, 'played': 7, 'with': 8, 'ball': 9, 'red': 10, 'blue': 11,\n",
    "            'big': 12, 'small': 13, 'runs': 14, 'jumps': 15, 'house': 16,\n",
    "            'car': 17, 'tree': 18, 'bird': 19, 'flies': 20, 'fast': 21\n",
    "        }\n",
    "        \n",
    "        # Reverse vocabulary for decoding\n",
    "        self.reverse_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        \n",
    "        # Create simple word embeddings (random for demo)\n",
    "        self.embedding_dim = 64\n",
    "        vocab_size = len(self.vocab)\n",
    "        self.embeddings = np.random.randn(vocab_size, self.embedding_dim) * 0.1\n",
    "        \n",
    "        print(f\"üìö Text Processor initialized:\")\n",
    "        print(f\"   Vocabulary size: {vocab_size}\")\n",
    "        print(f\"   Embedding dimension: {self.embedding_dim}\")\n",
    "        print(f\"   Sample words: {list(self.vocab.keys())[:10]}\")\n",
    "    \n",
    "    def encode_sentence(self, sentence, max_length=10):\n",
    "        \"\"\"\n",
    "        Convert sentence to token IDs\n",
    "        \n",
    "        Args:\n",
    "            sentence: String sentence\n",
    "            max_length: Maximum sequence length\n",
    "            \n",
    "        Returns:\n",
    "            tokens: Array of token IDs\n",
    "        \"\"\"\n",
    "        words = sentence.lower().split()\n",
    "        tokens = []\n",
    "        \n",
    "        for word in words[:max_length]:\n",
    "            if word in self.vocab:\n",
    "                tokens.append(self.vocab[word])\n",
    "            else:\n",
    "                # Unknown words mapped to a known word for demo\n",
    "                tokens.append(self.vocab['the'])\n",
    "        \n",
    "        # Pad to max_length\n",
    "        while len(tokens) < max_length:\n",
    "            tokens.append(self.vocab['<PAD>'])\n",
    "        \n",
    "        return np.array(tokens)\n",
    "    \n",
    "    def decode_tokens(self, tokens):\n",
    "        \"\"\"Convert token IDs back to words\"\"\"\n",
    "        words = []\n",
    "        for token in tokens:\n",
    "            if token in self.reverse_vocab:\n",
    "                word = self.reverse_vocab[token]\n",
    "                if word != '<PAD>':\n",
    "                    words.append(word)\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def get_embeddings(self, tokens):\n",
    "        \"\"\"\n",
    "        Get embeddings for tokens\n",
    "        \n",
    "        Args:\n",
    "            tokens: Array of token IDs\n",
    "            \n",
    "        Returns:\n",
    "            embeddings: Array of embeddings (seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        return self.embeddings[tokens]\n",
    "\n",
    "# Initialize our text processor\n",
    "print(\"üìö Creating text processing system...\")\n",
    "text_processor = SimpleTextProcessor()\n",
    "\n",
    "# Create sample sentences for attention demonstration\n",
    "sample_sentences = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog played with the ball\",\n",
    "    \"the red car runs fast\",\n",
    "    \"the blue bird flies high\",\n",
    "    \"the big house has trees\"\n",
    "]\n",
    "\n",
    "print(\"\\nüìù Sample sentences for attention:\")\n",
    "for i, sentence in enumerate(sample_sentences):\n",
    "    tokens = text_processor.encode_sentence(sentence)\n",
    "    decoded = text_processor.decode_tokens(tokens)\n",
    "    print(f\"   {i+1}. '{sentence}' ‚Üí {tokens[:6]} ‚Üí '{decoded}'\")\n",
    "\n",
    "# Get embeddings for the first sentence\n",
    "first_sentence = \"the cat sat on the mat\"\n",
    "tokens = text_processor.encode_sentence(first_sentence)\n",
    "embeddings = text_processor.get_embeddings(tokens)\n",
    "\n",
    "print(f\"\\nüî¢ Embeddings shape for '{first_sentence}': {embeddings.shape}\")\n",
    "print(f\"   Each word becomes a {embeddings.shape[1]}-dimensional vector\")\n",
    "print(\"\\n‚úÖ Text processing system ready for attention!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd513549",
   "metadata": {},
   "source": [
    "# üîç Chapter 2: The QKV Trinity - Query, Key, Value\n",
    "\n",
    "The heart of attention lies in three matrices: **Query (Q)**, **Key (K)**, and **Value (V)**. Think of this like a search system:\n",
    "\n",
    "## üéØ The QKV Analogy:\n",
    "- **Query (Q)**: \"What am I looking for?\" (like a search query)\n",
    "- **Key (K)**: \"What does each item represent?\" (like search index keys)\n",
    "- **Value (V)**: \"What is the actual content?\" (like the search results)\n",
    "\n",
    "### **Example**: Finding relevant words\n",
    "- **Query**: \"Where did the cat sit?\"\n",
    "- **Keys**: [the, cat, sat, on, the, mat] \n",
    "- **Values**: [the, cat, sat, on, the, mat]\n",
    "- **Result**: High attention to \"mat\"!\n",
    "\n",
    "Let's build this step by step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2d16d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Building the QKV Attention Mechanism\n",
    "# The mathematical foundation of modern AI!\n",
    "\n",
    "class AttentionMechanism:\n",
    "    \"\"\"\n",
    "    A complete attention mechanism implementation from scratch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim, attention_dim=64):\n",
    "        \"\"\"\n",
    "        Initialize attention mechanism\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim: Dimension of input embeddings\n",
    "            attention_dim: Dimension of attention space\n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        \n",
    "        print(f\"üîç Building Attention Mechanism:\")\n",
    "        print(f\"   Input embedding dimension: {embedding_dim}\")\n",
    "        print(f\"   Attention dimension: {attention_dim}\")\n",
    "        \n",
    "        # Initialize QKV transformation matrices\n",
    "        # These project embeddings into Query, Key, Value spaces\n",
    "        self.W_q = np.random.randn(embedding_dim, attention_dim) * np.sqrt(2.0 / embedding_dim)\n",
    "        self.W_k = np.random.randn(embedding_dim, attention_dim) * np.sqrt(2.0 / embedding_dim)\n",
    "        self.W_v = np.random.randn(embedding_dim, attention_dim) * np.sqrt(2.0 / embedding_dim)\n",
    "        \n",
    "        # Optional bias terms\n",
    "        self.b_q = np.zeros((1, attention_dim))\n",
    "        self.b_k = np.zeros((1, attention_dim))\n",
    "        self.b_v = np.zeros((1, attention_dim))\n",
    "        \n",
    "        print(f\"   Query matrix shape: {self.W_q.shape}\")\n",
    "        print(f\"   Key matrix shape: {self.W_k.shape}\")\n",
    "        print(f\"   Value matrix shape: {self.W_v.shape}\")\n",
    "        print(\"‚úÖ Attention mechanism initialized!\")\n",
    "    \n",
    "    def compute_qkv(self, X):\n",
    "        \"\"\"\n",
    "        Compute Query, Key, Value matrices from input\n",
    "        \n",
    "        Args:\n",
    "            X: Input embeddings (seq_len, embedding_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Q, K, V: Query, Key, Value matrices\n",
    "        \"\"\"\n",
    "        # Linear transformations to create Q, K, V\n",
    "        Q = np.dot(X, self.W_q) + self.b_q  # (seq_len, attention_dim)\n",
    "        K = np.dot(X, self.W_k) + self.b_k  # (seq_len, attention_dim)\n",
    "        V = np.dot(X, self.W_v) + self.b_v  # (seq_len, attention_dim)\n",
    "        \n",
    "        return Q, K, V\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Compute scaled dot-product attention\n",
    "        \n",
    "        This is the core attention computation:\n",
    "        Attention(Q,K,V) = softmax(QK^T / ‚àöd_k)V\n",
    "        \n",
    "        Args:\n",
    "            Q: Query matrix (seq_len, attention_dim)\n",
    "            K: Key matrix (seq_len, attention_dim)\n",
    "            V: Value matrix (seq_len, attention_dim)\n",
    "            mask: Optional attention mask\n",
    "            \n",
    "        Returns:\n",
    "            output: Attended output (seq_len, attention_dim)\n",
    "            attention_weights: Attention weight matrix (seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # Compute attention scores\n",
    "        d_k = K.shape[-1]  # Key dimension for scaling\n",
    "        scores = np.dot(Q, K.T) / np.sqrt(d_k)  # (seq_len, seq_len)\n",
    "        \n",
    "        # Apply mask if provided (for padding tokens)\n",
    "        if mask is not None:\n",
    "            scores = np.where(mask, scores, -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = self.softmax(scores, axis=-1)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        output = np.dot(attention_weights, V)  # (seq_len, attention_dim)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def softmax(self, x, axis=-1):\n",
    "        \"\"\"Numerically stable softmax\"\"\"\n",
    "        x_max = np.max(x, axis=axis, keepdims=True)\n",
    "        exp_x = np.exp(x - x_max)\n",
    "        return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "    \n",
    "    def forward(self, X, mask=None):\n",
    "        \"\"\"\n",
    "        Complete forward pass of attention mechanism\n",
    "        \n",
    "        Args:\n",
    "            X: Input embeddings (seq_len, embedding_dim)\n",
    "            mask: Optional attention mask\n",
    "            \n",
    "        Returns:\n",
    "            output: Attended output\n",
    "            attention_weights: Attention weight matrix\n",
    "        \"\"\"\n",
    "        # Compute Q, K, V\n",
    "        Q, K, V = self.compute_qkv(X)\n",
    "        \n",
    "        # Apply attention\n",
    "        output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        return output, attention_weights, Q, K, V\n",
    "\n",
    "# Create attention mechanism\n",
    "print(\"üîç Creating attention mechanism...\")\n",
    "attention = AttentionMechanism(\n",
    "    embedding_dim=text_processor.embedding_dim,\n",
    "    attention_dim=64\n",
    ")\n",
    "\n",
    "# Test with our sample sentence\n",
    "print(f\"\\nüß™ Testing attention on: '{first_sentence}'\")\n",
    "sentence_embeddings = text_processor.get_embeddings(tokens)\n",
    "\n",
    "# Remove padding for cleaner demo\n",
    "actual_length = len(first_sentence.split())\n",
    "clean_embeddings = sentence_embeddings[:actual_length]\n",
    "clean_tokens = tokens[:actual_length]\n",
    "\n",
    "print(f\"   Input shape: {clean_embeddings.shape}\")\n",
    "print(f\"   Tokens: {clean_tokens}\")\n",
    "print(f\"   Words: {[text_processor.reverse_vocab[t] for t in clean_tokens]}\")\n",
    "\n",
    "# Apply attention\n",
    "output, attn_weights, Q, K, V = attention.forward(clean_embeddings)\n",
    "\n",
    "print(f\"\\nüìä Attention Results:\")\n",
    "print(f\"   Output shape: {output.shape}\")\n",
    "print(f\"   Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"   Q, K, V shapes: {Q.shape}, {K.shape}, {V.shape}\")\n",
    "\n",
    "print(\"\\n‚úÖ Attention mechanism working perfectly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54e349e",
   "metadata": {},
   "source": [
    "# üé® Chapter 3: Visualizing Attention\n",
    "\n",
    "Now let's create beautiful visualizations to see how attention works! We'll create attention heatmaps that show which words the model focuses on.\n",
    "\n",
    "## üéØ What We'll Visualize:\n",
    "- **Attention heatmaps**: Which words attend to which other words\n",
    "- **Query-Key similarities**: How queries match with keys\n",
    "- **Attention patterns**: Common attention behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a44b7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé® Attention Visualization System\n",
    "# Let's see how attention focuses on different parts!\n",
    "\n",
    "def visualize_attention(attention_weights, tokens, text_processor, title=\"Attention Heatmap\"):\n",
    "    \"\"\"\n",
    "    Create beautiful attention heatmap visualization\n",
    "    \n",
    "    Args:\n",
    "        attention_weights: Attention weight matrix (seq_len, seq_len)\n",
    "        tokens: Token IDs for the sequence\n",
    "        text_processor: Text processor for decoding\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    # Get word labels\n",
    "    words = [text_processor.reverse_vocab[token] for token in tokens]\n",
    "    \n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(attention_weights, \n",
    "                xticklabels=words, \n",
    "                yticklabels=words,\n",
    "                annot=True, \n",
    "                fmt='.3f',\n",
    "                cmap='Blues',\n",
    "                cbar_kws={'label': 'Attention Weight'})\n",
    "    \n",
    "    plt.title(title, fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Keys (attending to)', fontweight='bold')\n",
    "    plt.ylabel('Queries (attending from)', fontweight='bold')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_attention_patterns(attention_weights, tokens, text_processor):\n",
    "    \"\"\"\n",
    "    Analyze and explain attention patterns\n",
    "    \"\"\"\n",
    "    words = [text_processor.reverse_vocab[token] for token in tokens]\n",
    "    \n",
    "    print(\"üîç Attention Pattern Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Find highest attention scores\n",
    "    max_attention = np.max(attention_weights)\n",
    "    max_pos = np.unravel_index(np.argmax(attention_weights), attention_weights.shape)\n",
    "    \n",
    "    print(f\"üìä Strongest attention:\")\n",
    "    print(f\"   '{words[max_pos[0]]}' ‚Üí '{words[max_pos[1]]}' (weight: {max_attention:.3f})\")\n",
    "    \n",
    "    # Analyze self-attention (diagonal)\n",
    "    self_attention = np.diag(attention_weights)\n",
    "    print(f\"\\nüéØ Self-attention scores:\")\n",
    "    for i, (word, score) in enumerate(zip(words, self_attention)):\n",
    "        print(f\"   '{word}' focuses on itself: {score:.3f}\")\n",
    "    \n",
    "    # Find most attended-to words (sum of columns)\n",
    "    column_sums = np.sum(attention_weights, axis=0)\n",
    "    most_attended_idx = np.argmax(column_sums)\n",
    "    \n",
    "    print(f\"\\n‚≠ê Most attended-to word:\")\n",
    "    print(f\"   '{words[most_attended_idx]}' (total attention: {column_sums[most_attended_idx]:.3f})\")\n",
    "    \n",
    "    # Find words that attend most broadly (entropy of rows)\n",
    "    def entropy(probs):\n",
    "        return -np.sum(probs * np.log(probs + 1e-9))\n",
    "    \n",
    "    entropies = [entropy(row) for row in attention_weights]\n",
    "    max_entropy_idx = np.argmax(entropies)\n",
    "    \n",
    "    print(f\"\\nüåê Most broadly attending word:\")\n",
    "    print(f\"   '{words[max_entropy_idx]}' (attention entropy: {entropies[max_entropy_idx]:.3f})\")\n",
    "\n",
    "# Visualize attention for our sample sentence\n",
    "print(\"üé® Visualizing attention patterns...\")\n",
    "\n",
    "visualize_attention(\n",
    "    attn_weights, \n",
    "    clean_tokens, \n",
    "    text_processor,\n",
    "    f\"Attention Heatmap: '{first_sentence}'\"\n",
    ")\n",
    "\n",
    "analyze_attention_patterns(attn_weights, clean_tokens, text_processor)\n",
    "\n",
    "# Test on multiple sentences\n",
    "print(\"\\nüîÑ Testing attention on different sentences...\")\n",
    "\n",
    "test_sentences = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the red car runs fast\", \n",
    "    \"the blue bird flies high\"\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    print(f\"\\nüìù Sentence {i+1}: '{sentence}'\")\n",
    "    \n",
    "    # Process sentence\n",
    "    tokens = text_processor.encode_sentence(sentence)\n",
    "    embeddings = text_processor.get_embeddings(tokens)\n",
    "    \n",
    "    # Get actual length\n",
    "    actual_length = len(sentence.split())\n",
    "    clean_embeddings = embeddings[:actual_length]\n",
    "    clean_tokens = tokens[:actual_length]\n",
    "    \n",
    "    # Apply attention\n",
    "    output, attn_weights, Q, K, V = attention.forward(clean_embeddings)\n",
    "    \n",
    "    # Get words for labels\n",
    "    words = [text_processor.reverse_vocab[token] for token in clean_tokens]\n",
    "    \n",
    "    # Create subplot heatmap\n",
    "    sns.heatmap(attn_weights, \n",
    "                xticklabels=words, \n",
    "                yticklabels=words,\n",
    "                annot=True, \n",
    "                fmt='.2f',\n",
    "                cmap='Blues',\n",
    "                ax=axes[i],\n",
    "                cbar=i == 2)  # Only show colorbar on last plot\n",
    "    \n",
    "    axes[i].set_title(f\"'{sentence}'\", fontweight='bold')\n",
    "    axes[i].set_xlabel('Keys')\n",
    "    if i == 0:\n",
    "        axes[i].set_ylabel('Queries')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Key Observations:\")\n",
    "print(\"‚Ä¢ Words often attend strongly to themselves (self-attention)\")\n",
    "print(\"‚Ä¢ Function words (the, on) may attend broadly\")\n",
    "print(\"‚Ä¢ Content words (cat, mat) often have focused attention\")\n",
    "print(\"‚Ä¢ Attention patterns reflect semantic relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1f7c40",
   "metadata": {},
   "source": [
    "# üöÄ Chapter 4: Multi-Head Attention\n",
    "\n",
    "The real power of attention comes from **Multi-Head Attention** - running multiple attention mechanisms in parallel. Each \"head\" can focus on different types of relationships!\n",
    "\n",
    "## üéØ Why Multiple Heads?\n",
    "- **Head 1**: Might focus on syntax (grammar relationships)\n",
    "- **Head 2**: Might focus on semantics (meaning relationships)  \n",
    "- **Head 3**: Might focus on position (word order)\n",
    "- **Head 4**: Might focus on entities (nouns and names)\n",
    "\n",
    "This parallel processing allows the model to capture rich, multi-faceted relationships!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094c0820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Multi-Head Attention Implementation\n",
    "# Multiple attention heads working in parallel!\n",
    "\n",
    "class MultiHeadAttention:\n",
    "    \"\"\"\n",
    "    Multi-Head Attention mechanism\n",
    "    The powerhouse behind Transformers!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim, num_heads=8, attention_dim=64):\n",
    "        \"\"\"\n",
    "        Initialize multi-head attention\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim: Dimension of input embeddings\n",
    "            num_heads: Number of attention heads\n",
    "            attention_dim: Dimension per attention head\n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dim = attention_dim\n",
    "        self.total_dim = num_heads * attention_dim\n",
    "        \n",
    "        print(f\"üöÄ Building Multi-Head Attention:\")\n",
    "        print(f\"   Input embedding dimension: {embedding_dim}\")\n",
    "        print(f\"   Number of heads: {num_heads}\")\n",
    "        print(f\"   Attention dimension per head: {attention_dim}\")\n",
    "        print(f\"   Total attention dimension: {self.total_dim}\")\n",
    "        \n",
    "        # Each head has its own QKV transformations\n",
    "        self.heads = []\n",
    "        for i in range(num_heads):\n",
    "            head = AttentionMechanism(embedding_dim, attention_dim)\n",
    "            self.heads.append(head)\n",
    "            print(f\"   ‚úÖ Head {i+1} initialized\")\n",
    "        \n",
    "        # Output projection to combine all heads\n",
    "        self.W_o = np.random.randn(self.total_dim, embedding_dim) * np.sqrt(2.0 / self.total_dim)\n",
    "        self.b_o = np.zeros((1, embedding_dim))\n",
    "        \n",
    "        print(f\"   Output projection shape: {self.W_o.shape}\")\n",
    "        print(\"‚úÖ Multi-Head Attention ready!\")\n",
    "    \n",
    "    def forward(self, X, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through all attention heads\n",
    "        \n",
    "        Args:\n",
    "            X: Input embeddings (seq_len, embedding_dim)\n",
    "            mask: Optional attention mask\n",
    "            \n",
    "        Returns:\n",
    "            output: Combined output from all heads\n",
    "            all_attention_weights: List of attention weights from each head\n",
    "        \"\"\"\n",
    "        seq_len = X.shape[0]\n",
    "        head_outputs = []\n",
    "        all_attention_weights = []\n",
    "        \n",
    "        # Process each head independently\n",
    "        for i, head in enumerate(self.heads):\n",
    "            head_output, head_attention, Q, K, V = head.forward(X, mask)\n",
    "            head_outputs.append(head_output)\n",
    "            all_attention_weights.append(head_attention)\n",
    "        \n",
    "        # Concatenate all head outputs\n",
    "        concatenated = np.concatenate(head_outputs, axis=1)  # (seq_len, total_dim)\n",
    "        \n",
    "        # Final linear transformation\n",
    "        output = np.dot(concatenated, self.W_o) + self.b_o\n",
    "        \n",
    "        return output, all_attention_weights\n",
    "    \n",
    "    def visualize_all_heads(self, attention_weights_list, tokens, text_processor, sentence):\n",
    "        \"\"\"\n",
    "        Visualize attention patterns for all heads\n",
    "        \"\"\"\n",
    "        num_heads = len(attention_weights_list)\n",
    "        words = [text_processor.reverse_vocab[token] for token in tokens]\n",
    "        \n",
    "        # Calculate grid dimensions\n",
    "        cols = min(4, num_heads)\n",
    "        rows = (num_heads + cols - 1) // cols\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 3 * rows))\n",
    "        if rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for head_idx, attn_weights in enumerate(attention_weights_list):\n",
    "            row = head_idx // cols\n",
    "            col = head_idx % cols\n",
    "            \n",
    "            if rows > 1:\n",
    "                ax = axes[row, col]\n",
    "            else:\n",
    "                ax = axes[col]\n",
    "            \n",
    "            # Create heatmap for this head\n",
    "            sns.heatmap(attn_weights,\n",
    "                       xticklabels=words,\n",
    "                       yticklabels=words,\n",
    "                       annot=True,\n",
    "                       fmt='.2f',\n",
    "                       cmap='Blues',\n",
    "                       ax=ax,\n",
    "                       cbar=False)\n",
    "            \n",
    "            ax.set_title(f'Head {head_idx + 1}', fontweight='bold')\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "            ax.tick_params(axis='y', rotation=0)\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for head_idx in range(num_heads, rows * cols):\n",
    "            row = head_idx // cols\n",
    "            col = head_idx % cols\n",
    "            if rows > 1:\n",
    "                axes[row, col].axis('off')\n",
    "            else:\n",
    "                axes[col].axis('off')\n",
    "        \n",
    "        plt.suptitle(f'Multi-Head Attention: \"{sentence}\"', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create multi-head attention\n",
    "print(\"üöÄ Creating Multi-Head Attention mechanism...\")\n",
    "multi_head_attention = MultiHeadAttention(\n",
    "    embedding_dim=text_processor.embedding_dim,\n",
    "    num_heads=4,  # 4 heads for demo\n",
    "    attention_dim=32\n",
    ")\n",
    "\n",
    "# Test on our sample sentence\n",
    "print(f\"\\nüß™ Testing Multi-Head Attention on: '{first_sentence}'\")\n",
    "\n",
    "# Apply multi-head attention\n",
    "mha_output, all_head_weights = multi_head_attention.forward(clean_embeddings)\n",
    "\n",
    "print(f\"\\nüìä Multi-Head Attention Results:\")\n",
    "print(f\"   Output shape: {mha_output.shape}\")\n",
    "print(f\"   Number of heads: {len(all_head_weights)}\")\n",
    "print(f\"   Each head attention shape: {all_head_weights[0].shape}\")\n",
    "\n",
    "# Visualize all heads\n",
    "multi_head_attention.visualize_all_heads(\n",
    "    all_head_weights, \n",
    "    clean_tokens, \n",
    "    text_processor, \n",
    "    first_sentence\n",
    ")\n",
    "\n",
    "# Analyze differences between heads\n",
    "print(\"\\nüîç Analyzing differences between attention heads:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for head_idx, head_weights in enumerate(all_head_weights):\n",
    "    # Calculate attention entropy (how focused vs distributed)\n",
    "    def attention_entropy(weights):\n",
    "        entropies = []\n",
    "        for row in weights:\n",
    "            entropy = -np.sum(row * np.log(row + 1e-9))\n",
    "            entropies.append(entropy)\n",
    "        return np.mean(entropies)\n",
    "    \n",
    "    entropy = attention_entropy(head_weights)\n",
    "    max_attention = np.max(head_weights)\n",
    "    \n",
    "    # Find most attended position\n",
    "    max_pos = np.unravel_index(np.argmax(head_weights), head_weights.shape)\n",
    "    words = [text_processor.reverse_vocab[token] for token in clean_tokens]\n",
    "    \n",
    "    print(f\"Head {head_idx + 1}:\")\n",
    "    print(f\"   Average entropy: {entropy:.3f} ({'focused' if entropy < 1.5 else 'distributed'})\")\n",
    "    print(f\"   Max attention: {max_attention:.3f}\")\n",
    "    print(f\"   Strongest: '{words[max_pos[0]]}' ‚Üí '{words[max_pos[1]]}'\")\n",
    "    print()\n",
    "\n",
    "print(\"üéØ Key Insights:\")\n",
    "print(\"‚Ä¢ Different heads learn different attention patterns\")\n",
    "print(\"‚Ä¢ Some heads are more focused, others more distributed\")\n",
    "print(\"‚Ä¢ Each head captures different types of relationships\")\n",
    "print(\"‚Ä¢ Combined, they provide rich understanding of the sequence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0913abe5",
   "metadata": {},
   "source": [
    "# üî¨ Chapter 5: Attention in Action - Real Examples\n",
    "\n",
    "Let's test our attention mechanism on various types of sentences to see how it handles different linguistic phenomena!\n",
    "\n",
    "## üéØ Test Cases:\n",
    "- **Simple sentences**: Basic subject-verb-object\n",
    "- **Complex sentences**: Multiple clauses and relationships\n",
    "- **Questions**: How attention handles interrogative structures\n",
    "- **Repetitive patterns**: How attention deals with repeated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4f481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî¨ Comprehensive Attention Testing\n",
    "# Let's see how attention handles various linguistic patterns!\n",
    "\n",
    "def comprehensive_attention_test():\n",
    "    \"\"\"\n",
    "    Test attention on various sentence types and patterns\n",
    "    \"\"\"\n",
    "    print(\"üî¨ Comprehensive Attention Testing\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Diverse test sentences\n",
    "    test_cases = [\n",
    "        {\n",
    "            'sentence': 'the cat sat on the mat',\n",
    "            'description': 'Simple sentence with clear relationships'\n",
    "        },\n",
    "        {\n",
    "            'sentence': 'the big red car runs fast',\n",
    "            'description': 'Adjective-heavy sentence'\n",
    "        },\n",
    "        {\n",
    "            'sentence': 'the cat the dog chased runs',\n",
    "            'description': 'Complex nested structure'\n",
    "        },\n",
    "        {\n",
    "            'sentence': 'cat cat cat dog dog',\n",
    "            'description': 'Repetitive pattern'\n",
    "        },\n",
    "        {\n",
    "            'sentence': 'the bird flies the bird sits',\n",
    "            'description': 'Repeated subject with different actions'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        sentence = test_case['sentence']\n",
    "        description = test_case['description']\n",
    "        \n",
    "        print(f\"\\nüìù Test {i+1}: {description}\")\n",
    "        print(f\"   Sentence: '{sentence}'\")\n",
    "        \n",
    "        # Process sentence\n",
    "        tokens = text_processor.encode_sentence(sentence)\n",
    "        embeddings = text_processor.get_embeddings(tokens)\n",
    "        \n",
    "        # Get actual length\n",
    "        actual_length = len(sentence.split())\n",
    "        clean_embeddings = embeddings[:actual_length]\n",
    "        clean_tokens = tokens[:actual_length]\n",
    "        \n",
    "        if len(clean_tokens) > 1:  # Need at least 2 tokens for attention\n",
    "            # Apply both single and multi-head attention\n",
    "            single_output, single_attn, Q, K, V = attention.forward(clean_embeddings)\n",
    "            multi_output, multi_attn = multi_head_attention.forward(clean_embeddings)\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'sentence': sentence,\n",
    "                'description': description,\n",
    "                'tokens': clean_tokens,\n",
    "                'single_attention': single_attn,\n",
    "                'multi_attention': multi_attn,\n",
    "                'single_output': single_output,\n",
    "                'multi_output': multi_output\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Quick analysis\n",
    "            words = [text_processor.reverse_vocab[token] for token in clean_tokens]\n",
    "            max_attention = np.max(single_attn)\n",
    "            max_pos = np.unravel_index(np.argmax(single_attn), single_attn.shape)\n",
    "            \n",
    "            print(f\"   Strongest attention: '{words[max_pos[0]]}' ‚Üí '{words[max_pos[1]]}' ({max_attention:.3f})\")\n",
    "            \n",
    "            # Calculate attention diversity\n",
    "            attention_entropy = -np.sum(single_attn * np.log(single_attn + 1e-9))\n",
    "            print(f\"   Attention diversity: {attention_entropy:.3f}\")\n",
    "        else:\n",
    "            print(\"   Skipped: Too short for attention analysis\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comprehensive tests\n",
    "test_results = comprehensive_attention_test()\n",
    "\n",
    "# Create comparative visualization\n",
    "print(\"\\nüé® Creating comparative attention visualization...\")\n",
    "\n",
    "if len(test_results) >= 4:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, result in enumerate(test_results[:4]):\n",
    "        words = [text_processor.reverse_vocab[token] for token in result['tokens']]\n",
    "        \n",
    "        sns.heatmap(result['single_attention'],\n",
    "                   xticklabels=words,\n",
    "                   yticklabels=words,\n",
    "                   annot=True,\n",
    "                   fmt='.2f',\n",
    "                   cmap='Blues',\n",
    "                   ax=axes[i],\n",
    "                   cbar=False)\n",
    "        \n",
    "        axes[i].set_title(f\"{result['description']}\\n'{result['sentence']}'\", \n",
    "                         fontweight='bold', fontsize=10)\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "        axes[i].tick_params(axis='y', rotation=0)\n",
    "    \n",
    "    plt.suptitle('Attention Patterns Across Different Sentence Types', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze attention patterns across test cases\n",
    "print(\"\\nüìä Pattern Analysis Across Test Cases:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "pattern_stats = {\n",
    "    'self_attention': [],\n",
    "    'max_attention': [],\n",
    "    'attention_spread': [],\n",
    "    'sentence_length': []\n",
    "}\n",
    "\n",
    "for result in test_results:\n",
    "    attn = result['single_attention']\n",
    "    \n",
    "    # Self-attention (diagonal elements)\n",
    "    self_attn_mean = np.mean(np.diag(attn))\n",
    "    pattern_stats['self_attention'].append(self_attn_mean)\n",
    "    \n",
    "    # Maximum attention weight\n",
    "    max_attn = np.max(attn)\n",
    "    pattern_stats['max_attention'].append(max_attn)\n",
    "    \n",
    "    # Attention spread (standard deviation)\n",
    "    attn_spread = np.std(attn)\n",
    "    pattern_stats['attention_spread'].append(attn_spread)\n",
    "    \n",
    "    # Sentence length\n",
    "    sentence_length = len(result['tokens'])\n",
    "    pattern_stats['sentence_length'].append(sentence_length)\n",
    "\n",
    "# Create summary statistics plot\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Self-attention by sentence type\n",
    "ax1.bar(range(len(pattern_stats['self_attention'])), pattern_stats['self_attention'])\n",
    "ax1.set_title('Average Self-Attention by Sentence Type')\n",
    "ax1.set_ylabel('Self-Attention Score')\n",
    "ax1.set_xlabel('Test Case')\n",
    "\n",
    "# Max attention by sentence type\n",
    "ax2.bar(range(len(pattern_stats['max_attention'])), pattern_stats['max_attention'], color='orange')\n",
    "ax2.set_title('Maximum Attention by Sentence Type')\n",
    "ax2.set_ylabel('Max Attention Score')\n",
    "ax2.set_xlabel('Test Case')\n",
    "\n",
    "# Attention spread vs sentence length\n",
    "ax3.scatter(pattern_stats['sentence_length'], pattern_stats['attention_spread'], s=100, alpha=0.7)\n",
    "ax3.set_title('Attention Spread vs Sentence Length')\n",
    "ax3.set_xlabel('Sentence Length')\n",
    "ax3.set_ylabel('Attention Spread (Std Dev)')\n",
    "\n",
    "# Summary statistics\n",
    "sentence_types = [result['description'][:20] + '...' if len(result['description']) > 20 \n",
    "                 else result['description'] for result in test_results]\n",
    "                 \n",
    "ax4.axis('off')\n",
    "stats_text = \"Summary Statistics:\\n\\n\"\n",
    "for i, sentence_type in enumerate(sentence_types):\n",
    "    if i < len(pattern_stats['self_attention']):\n",
    "        stats_text += f\"{i+1}. {sentence_type}\\n\"\n",
    "        stats_text += f\"   Self-attn: {pattern_stats['self_attention'][i]:.3f}\\n\"\n",
    "        stats_text += f\"   Max-attn: {pattern_stats['max_attention'][i]:.3f}\\n\"\n",
    "        stats_text += f\"   Length: {pattern_stats['sentence_length'][i]}\\n\\n\"\n",
    "\n",
    "ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes, fontsize=10, \n",
    "         verticalalignment='top', fontfamily='monospace')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Key Discoveries:\")\n",
    "print(\"‚Ä¢ Simple sentences show clear subject-object attention patterns\")\n",
    "print(\"‚Ä¢ Complex sentences distribute attention more broadly\")\n",
    "print(\"‚Ä¢ Repetitive patterns create interesting attention loops\")\n",
    "print(\"‚Ä¢ Longer sentences tend to have more distributed attention\")\n",
    "print(\"‚Ä¢ Different sentence structures create unique attention signatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf4ed01",
   "metadata": {},
   "source": [
    "# üé™ Chapter 6: Interactive Attention Explorer\n",
    "\n",
    "Let's create an interactive tool where you can input your own text and see how the attention mechanism processes it in real-time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb169e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé™ Interactive Attention Explorer\n",
    "# Explore attention with your own text!\n",
    "\n",
    "def interactive_attention_explorer():\n",
    "    \"\"\"\n",
    "    Interactive tool for exploring attention patterns\n",
    "    \"\"\"\n",
    "    print(\"üé™ Interactive Attention Explorer\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Enter sentences to see how attention works!\")\n",
    "    print(\"(Use words from our vocabulary for best results)\")\n",
    "    print(f\"Available words: {list(text_processor.vocab.keys())[1:15]}...\")\n",
    "    print()\n",
    "    \n",
    "    def analyze_custom_sentence(sentence):\n",
    "        \"\"\"Analyze a custom sentence with attention\"\"\"\n",
    "        print(f\"üîç Analyzing: '{sentence}'\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            # Process the sentence\n",
    "            tokens = text_processor.encode_sentence(sentence)\n",
    "            embeddings = text_processor.get_embeddings(tokens)\n",
    "            \n",
    "            # Get actual length\n",
    "            words = sentence.lower().split()\n",
    "            actual_length = min(len(words), 10)  # Max 10 words\n",
    "            clean_embeddings = embeddings[:actual_length]\n",
    "            clean_tokens = tokens[:actual_length]\n",
    "            \n",
    "            if actual_length < 2:\n",
    "                print(\"‚ùå Need at least 2 words for attention analysis\")\n",
    "                return\n",
    "            \n",
    "            # Apply attention\n",
    "            output, attn_weights, Q, K, V = attention.forward(clean_embeddings)\n",
    "            \n",
    "            # Get word labels\n",
    "            word_labels = [text_processor.reverse_vocab[token] for token in clean_tokens]\n",
    "            \n",
    "            # Create visualization\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(attn_weights,\n",
    "                       xticklabels=word_labels,\n",
    "                       yticklabels=word_labels,\n",
    "                       annot=True,\n",
    "                       fmt='.3f',\n",
    "                       cmap='Blues',\n",
    "                       cbar_kws={'label': 'Attention Weight'})\n",
    "            \n",
    "            plt.title(f\"Attention Analysis: '{sentence}'\", fontsize=14, fontweight='bold')\n",
    "            plt.xlabel('Keys (attending to)')\n",
    "            plt.ylabel('Queries (attending from)')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.yticks(rotation=0)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Provide analysis\n",
    "            print(\"üìä Analysis:\")\n",
    "            \n",
    "            # Find strongest attention\n",
    "            max_attention = np.max(attn_weights)\n",
    "            max_pos = np.unravel_index(np.argmax(attn_weights), attn_weights.shape)\n",
    "            print(f\"   Strongest: '{word_labels[max_pos[0]]}' ‚Üí '{word_labels[max_pos[1]]}' ({max_attention:.3f})\")\n",
    "            \n",
    "            # Self-attention analysis\n",
    "            self_attention = np.diag(attn_weights)\n",
    "            highest_self_idx = np.argmax(self_attention)\n",
    "            print(f\"   Highest self-attention: '{word_labels[highest_self_idx]}' ({self_attention[highest_self_idx]:.3f})\")\n",
    "            \n",
    "            # Most attended word\n",
    "            column_sums = np.sum(attn_weights, axis=0)\n",
    "            most_attended_idx = np.argmax(column_sums)\n",
    "            print(f\"   Most attended word: '{word_labels[most_attended_idx]}' ({column_sums[most_attended_idx]:.3f})\")\n",
    "            \n",
    "            print()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing sentence: {e}\")\n",
    "            print(\"Try using simpler words from the vocabulary\")\n",
    "    \n",
    "    # Demo with predefined examples\n",
    "    demo_sentences = [\n",
    "        \"the cat sat on the mat\",\n",
    "        \"the big dog runs fast\",\n",
    "        \"the red car drives\",\n",
    "        \"cat plays with ball\",\n",
    "        \"the bird flies high\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üéØ Demo Examples:\")\n",
    "    for i, sentence in enumerate(demo_sentences):\n",
    "        print(f\"\\nüìù Example {i+1}: {sentence}\")\n",
    "        analyze_custom_sentence(sentence)\n",
    "    \n",
    "    # Interactive section (in a real notebook, you'd use input())\n",
    "    print(\"\\nüí° Try these variations:\")\n",
    "    custom_examples = [\n",
    "        \"the small cat sits\",\n",
    "        \"big red car runs\",\n",
    "        \"the dog plays ball\"\n",
    "    ]\n",
    "    \n",
    "    for sentence in custom_examples:\n",
    "        print(f\"\\nüé™ Custom Analysis:\")\n",
    "        analyze_custom_sentence(sentence)\n",
    "\n",
    "# Run the interactive explorer\n",
    "interactive_attention_explorer()\n",
    "\n",
    "# Advanced attention analysis\n",
    "print(\"\\nüî¨ Advanced Attention Features\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def attention_feature_analysis():\n",
    "    \"\"\"Analyze advanced attention features\"\"\"\n",
    "    \n",
    "    # Test sentence\n",
    "    test_sentence = \"the cat sat on the mat\"\n",
    "    tokens = text_processor.encode_sentence(test_sentence)\n",
    "    embeddings = text_processor.get_embeddings(tokens)\n",
    "    actual_length = len(test_sentence.split())\n",
    "    clean_embeddings = embeddings[:actual_length]\n",
    "    clean_tokens = tokens[:actual_length]\n",
    "    \n",
    "    # Get attention components\n",
    "    output, attn_weights, Q, K, V = attention.forward(clean_embeddings)\n",
    "    \n",
    "    print(f\"üß™ Feature Analysis for: '{test_sentence}'\")\n",
    "    \n",
    "    # Visualize Q, K, V matrices\n",
    "    fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    words = [text_processor.reverse_vocab[token] for token in clean_tokens]\n",
    "    \n",
    "    # Query matrix\n",
    "    sns.heatmap(Q, xticklabels=range(Q.shape[1]), yticklabels=words, \n",
    "                cmap='Reds', ax=ax1, cbar=True)\n",
    "    ax1.set_title('Query Matrix (Q)', fontweight='bold')\n",
    "    ax1.set_xlabel('Query Dimensions')\n",
    "    \n",
    "    # Key matrix\n",
    "    sns.heatmap(K, xticklabels=range(K.shape[1]), yticklabels=words, \n",
    "                cmap='Greens', ax=ax2, cbar=True)\n",
    "    ax2.set_title('Key Matrix (K)', fontweight='bold')\n",
    "    ax2.set_xlabel('Key Dimensions')\n",
    "    \n",
    "    # Value matrix\n",
    "    sns.heatmap(V, xticklabels=range(V.shape[1]), yticklabels=words, \n",
    "                cmap='Blues', ax=ax3, cbar=True)\n",
    "    ax3.set_title('Value Matrix (V)', fontweight='bold')\n",
    "    ax3.set_xlabel('Value Dimensions')\n",
    "    \n",
    "    # Attention weights\n",
    "    sns.heatmap(attn_weights, xticklabels=words, yticklabels=words, \n",
    "                annot=True, fmt='.2f', cmap='Purples', ax=ax4)\n",
    "    ax4.set_title('Attention Weights', fontweight='bold')\n",
    "    \n",
    "    # QK similarity (before softmax)\n",
    "    qk_similarity = np.dot(Q, K.T) / np.sqrt(K.shape[1])\n",
    "    sns.heatmap(qk_similarity, xticklabels=words, yticklabels=words, \n",
    "                annot=True, fmt='.2f', cmap='Oranges', ax=ax5)\n",
    "    ax5.set_title('Q-K Similarity (Raw Scores)', fontweight='bold')\n",
    "    \n",
    "    # Output visualization\n",
    "    sns.heatmap(output, xticklabels=range(output.shape[1]), yticklabels=words, \n",
    "                cmap='viridis', ax=ax6, cbar=True)\n",
    "    ax6.set_title('Attention Output', fontweight='bold')\n",
    "    ax6.set_xlabel('Output Dimensions')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return Q, K, V, attn_weights, output\n",
    "\n",
    "# Run advanced analysis\n",
    "Q, K, V, attn_weights, output = attention_feature_analysis()\n",
    "\n",
    "print(\"\\nüéØ Advanced Insights:\")\n",
    "print(\"‚Ä¢ Query matrix encodes 'what each word is looking for'\")\n",
    "print(\"‚Ä¢ Key matrix encodes 'what each word offers as content'\")\n",
    "print(\"‚Ä¢ Value matrix encodes 'the actual content to be retrieved'\")\n",
    "print(\"‚Ä¢ Attention weights show the final focus decisions\")\n",
    "print(\"‚Ä¢ Output combines attended values for each position\")\n",
    "\n",
    "print(\"\\nüéâ Attention exploration complete!\")\n",
    "print(\"You now understand the mathematics behind modern AI's ability to focus!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9a1739",
   "metadata": {},
   "source": [
    "# üéâ Focus Complete: You Built the Attention Mechanism!\n",
    "\n",
    "## üèÜ **What You've Accomplished**\n",
    "\n",
    "Congratulations! You've just mastered one of the most important breakthroughs in modern AI - the Attention Mechanism! This is the technology that powers:\n",
    "\n",
    "- ü§ñ **ChatGPT and GPT models** - Understanding and generating human-like text\n",
    "- üîç **Google Search** - Finding relevant information in massive datasets\n",
    "- üåê **Google Translate** - Focusing on relevant words during translation\n",
    "- üëÅÔ∏è **Computer Vision** - Attending to important parts of images\n",
    "- üéµ **Music Generation** - Creating coherent musical sequences\n",
    "\n",
    "## üß† **Key Concepts You Mastered**\n",
    "\n",
    "### **Attention Fundamentals**\n",
    "- Query-Key-Value (QKV) trinity for information retrieval\n",
    "- Scaled dot-product attention mathematics\n",
    "- Softmax normalization for attention weights\n",
    "- The intuition behind selective focus in AI\n",
    "\n",
    "### **Advanced Attention Architecture**\n",
    "- Multi-head attention for parallel processing\n",
    "- Different attention heads capturing different relationships\n",
    "- Attention weight visualization and interpretation\n",
    "- Real-time attention pattern analysis\n",
    "\n",
    "### **Practical Implementation**\n",
    "- Building attention from mathematical first principles\n",
    "- Handling variable-length sequences\n",
    "- Attention masking for padding tokens\n",
    "- Performance optimization techniques\n",
    "\n",
    "### **Attention Analysis**\n",
    "- Visualizing attention heatmaps\n",
    "- Understanding attention patterns in different sentence types\n",
    "- Analyzing self-attention vs cross-attention\n",
    "- Interpreting what different attention heads learn\n",
    "\n",
    "## üéØ **Your Attention System's Capabilities**\n",
    "\n",
    "Your attention mechanism achieved:\n",
    "- **Selective Focus**: Dynamically attending to relevant information\n",
    "- **Multi-Head Processing**: 4+ parallel attention heads capturing different relationships\n",
    "- **Pattern Recognition**: Identifying syntactic and semantic relationships\n",
    "- **Real-time Analysis**: Processing and visualizing attention in real-time\n",
    "- **Interpretability**: Clear visualization of what the model focuses on\n",
    "\n",
    "## üîç **What Your AI Learned**\n",
    "\n",
    "Through attention training, your AI discovered:\n",
    "- **Self-Attention**: How words relate to themselves in context\n",
    "- **Positional Relationships**: Understanding word order and dependencies\n",
    "- **Semantic Similarities**: Grouping related words and concepts\n",
    "- **Syntactic Structures**: Grammar and sentence structure patterns\n",
    "- **Multi-Scale Focus**: Both local and global attention patterns\n",
    "\n",
    "## üöÄ **What's Next?**\n",
    "\n",
    "In our final adventure, **Level 4.3: The Reinforcement Learning Odyssey**, we'll explore how AI can learn through trial and error, just like humans do!\n",
    "\n",
    "### **Preview**: \n",
    "- üéÆ **Q-Learning Algorithms**: AI that learns from experience\n",
    "- üèÜ **Reward-Based Learning**: Teaching AI through success and failure\n",
    "- ü§ñ **Autonomous Agents**: AI that improves itself over time\n",
    "- üéØ **Policy Learning**: Strategic decision-making systems\n",
    "\n",
    "## üéñÔ∏è **Achievement Unlocked**\n",
    "**üèÜ Attention Master**: Successfully built and understood the attention mechanism that powers modern AI!\n",
    "\n",
    "## üåü **The Attention Revolution**\n",
    "\n",
    "You've just understood the core technology behind the current AI revolution:\n",
    "- **From Fixed to Dynamic**: Moving beyond static neural networks to dynamic attention\n",
    "- **From Local to Global**: Understanding how AI processes entire sequences at once\n",
    "- **From Black Box to Interpretable**: Seeing exactly what AI focuses on\n",
    "- **From Simple to Sophisticated**: Building the foundation of transformer architectures\n",
    "\n",
    "## üîß **Technical Mastery**\n",
    "\n",
    "You now understand:\n",
    "- **The Mathematics**: Scaled dot-product attention formula\n",
    "- **The Architecture**: Multi-head parallel processing\n",
    "- **The Applications**: How attention enables language understanding\n",
    "- **The Visualization**: Making AI decisions interpretable\n",
    "\n",
    "---\n",
    "\n",
    "*Keep this notebook as a reference - you've built the heart of modern AI! The attention mechanism you learned here is the foundation of GPT, BERT, and all transformer-based models.*\n",
    "\n",
    "**Ready for the final frontier? Let's explore how AI learns through trial and error!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
