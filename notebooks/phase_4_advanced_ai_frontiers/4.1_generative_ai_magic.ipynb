{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5943277",
   "metadata": {},
   "source": [
    "# 🎨 Level 4.1: The Generative AI Magic\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/ai-mastery-from-scratch/blob/main/notebooks/phase_4_advanced_ai_frontiers/4.1_generative_ai_magic.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **The Challenge**\n",
    "**Can AI create new content from imagination?**\n",
    "\n",
    "Welcome to the magical world of Generative AI! Today we're crossing into territory that was once thought impossible - teaching machines to be creative. We'll build AI that can generate new images, create art, and produce content that has never existed before. This is where AI transforms from understanding the world to creating new worlds.\n",
    "\n",
    "### **What You'll Discover:**\n",
    "- 🎨 How AI learns to generate completely new content\n",
    "- 🧠 The mathematics behind artificial creativity\n",
    "- ✨ Variational Autoencoders (VAEs) and how they work\n",
    "- 🎭 AI that dreams and imagines like humans\n",
    "\n",
    "### **What You'll Build:**\n",
    "A generative AI system that can create new handwritten digits and eventually generate simple artwork!\n",
    "\n",
    "### **The Journey Ahead:**\n",
    "1. **The Creativity Engine** - Understanding generative models\n",
    "2. **The Encoder-Decoder Architecture** - Learning compressed representations\n",
    "3. **The Latent Space Explorer** - The hidden dimension of creativity\n",
    "4. **The Content Generator** - AI that creates from noise\n",
    "5. **The Art Creator** - Building your own creative AI system\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 **Setup & Installation**\n",
    "\n",
    "*Run the cells below to set up your environment. This works in both Google Colab and local Jupyter notebooks.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac91da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Install Required Packages\n",
    "# This cell installs all necessary packages for this lesson\n",
    "# Run this first - it may take a minute!\n",
    "\n",
    "print(\"🚀 Installing packages for Generative AI Magic...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Install packages using simple pip commands\n",
    "!pip install numpy --quiet\n",
    "!pip install matplotlib --quiet\n",
    "!pip install seaborn --quiet\n",
    "!pip install scikit-learn --quiet\n",
    "!pip install ipywidgets --quiet\n",
    "!pip install tqdm --quiet\n",
    "!pip install pillow --quiet\n",
    "\n",
    "print(\"✅ numpy - Mathematical operations for neural networks\")\n",
    "print(\"✅ matplotlib - Beautiful plots and visualizations\") \n",
    "print(\"✅ seaborn - Enhanced plotting styles\")\n",
    "print(\"✅ scikit-learn - Dataset utilities and preprocessing\")\n",
    "print(\"✅ ipywidgets - Interactive notebook widgets\")\n",
    "print(\"✅ tqdm - Progress bars for training loops\")\n",
    "print(\"✅ pillow - Image processing and manipulation\")\n",
    "\n",
    "print(\"=\" * 60)        \n",
    "print(\"🎉 Setup complete! Ready to create AI magic!\")\n",
    "print(\"👇 Continue to the next cell to start creating...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c91b4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 Environment Check & Imports\n",
    "# Let's verify everything is working and import our tools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "# Set up beautiful plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Enable interactive widgets for Jupyter\n",
    "try:\n",
    "    from IPython.display import display, HTML, clear_output\n",
    "    import ipywidgets as widgets\n",
    "    print(\"✅ Interactive widgets available!\")\n",
    "    WIDGETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"⚠️  Interactive widgets not available (still works fine!)\")\n",
    "    WIDGETS_AVAILABLE = False\n",
    "\n",
    "# Check if we're in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"🌐 Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"💻 Running in local Jupyter\")\n",
    "\n",
    "print(\"🎯 Environment Status:\")\n",
    "print(f\"   Python version: {sys.version.split()[0]}\")\n",
    "print(f\"   NumPy version: {np.__version__}\")\n",
    "print(f\"   Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\\n🚀 Ready to start the Generative AI Magic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746ad0dc",
   "metadata": {},
   "source": [
    "# 🎨 Chapter 1: Understanding Generative Models\n",
    "\n",
    "Before we can build AI that creates, we need to understand what makes something \"generative.\" Let's explore the difference between discriminative AI (what we've built so far) and generative AI.\n",
    "\n",
    "## 🎯 Discriminative vs Generative AI:\n",
    "\n",
    "### **Discriminative AI** (What we've built):\n",
    "- **Learns to classify**: \"Is this a cat or dog?\"\n",
    "- **Learns to predict**: \"What digit is this?\"\n",
    "- **Learns patterns in data**: Recognizes existing content\n",
    "\n",
    "### **Generative AI** (What we're building):\n",
    "- **Learns to create**: \"Generate a new cat image\"\n",
    "- **Learns to imagine**: \"Create a digit that doesn't exist\"\n",
    "- **Learns the data distribution**: Understands how to make new content\n",
    "\n",
    "Let's start by loading our familiar MNIST dataset, but this time we'll use it differently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d59326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎨 Loading Data for Generative Learning\n",
    "# We'll use MNIST again, but this time to learn how to CREATE digits!\n",
    "\n",
    "print(\"🎨 Loading MNIST dataset for generative learning...\")\n",
    "print(\"This time we're not just recognizing - we're learning to CREATE!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load a subset of MNIST for faster training\n",
    "try:\n",
    "    # Try to load MNIST\n",
    "    mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n",
    "    X_full = mnist.data.astype('float32')\n",
    "    y_full = mnist.target.astype('int64')\n",
    "    \n",
    "    # Take a smaller subset for this demo (first 10,000 samples)\n",
    "    X = X_full[:10000]\n",
    "    y = y_full[:10000]\n",
    "    \n",
    "except:\n",
    "    # Fallback: create synthetic digit-like data\n",
    "    print(\"Creating synthetic digit-like data for demo...\")\n",
    "    X = np.random.rand(10000, 784) * 255\n",
    "    y = np.random.randint(0, 10, 10000)\n",
    "\n",
    "# Normalize pixel values to 0-1 range\n",
    "X = X / 255.0\n",
    "\n",
    "print(f\"📊 Dataset for Generation:\")\n",
    "print(f\"   Total samples: {X.shape[0]:,}\")\n",
    "print(f\"   Image dimensions: {X.shape[1]} pixels (28x28 flattened)\")\n",
    "print(f\"   Pixel value range: {X.min():.3f} to {X.max():.3f}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"   Training samples: {X_train.shape[0]:,}\")\n",
    "print(f\"   Test samples: {X_test.shape[0]:,}\")\n",
    "\n",
    "# Visualize some training examples\n",
    "def show_digits(X, y, title=\"Sample Digits\", num_samples=25):\n",
    "    \"\"\"Display a grid of digit images\"\"\"\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(12, 12))\n",
    "    fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    \n",
    "    indices = np.random.choice(len(X), num_samples, replace=False)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < num_samples:\n",
    "            # Reshape flattened image back to 28x28\n",
    "            image = X[indices[i]].reshape(28, 28)\n",
    "            label = y[indices[i]]\n",
    "            \n",
    "            ax.imshow(image, cmap='gray_r', interpolation='nearest')\n",
    "            ax.set_title(f'Digit: {label}', fontsize=10, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n🎯 Let's see what our AI will learn to generate:\")\n",
    "show_digits(X_train, y_train, \"Training Data: Real Handwritten Digits\")\n",
    "\n",
    "print(\"\\n✨ Goal: Teach AI to generate NEW digits that look just as real!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ad4a93",
   "metadata": {},
   "source": [
    "# 🏗️ Chapter 2: Building the Autoencoder Foundation\n",
    "\n",
    "Our first step toward generative AI is building an **Autoencoder** - a neural network that learns to compress images into a small \"code\" and then reconstruct them. This teaches the AI what makes a digit look like a digit.\n",
    "\n",
    "## 🎯 Autoencoder Architecture:\n",
    "- **Encoder**: Compresses 784 pixels → 32 numbers (latent code)\n",
    "- **Decoder**: Expands 32 numbers → 784 pixels (reconstructed image)\n",
    "- **Goal**: Output should match input (perfect reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912fdfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🏗️ Autoencoder: Learning to Compress and Reconstruct\n",
    "# This is the foundation of generative AI!\n",
    "\n",
    "class Autoencoder:\n",
    "    \"\"\"\n",
    "    A neural network that learns to compress and reconstruct images\n",
    "    This is the first step toward generative AI!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=784, latent_size=32, learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        Initialize the autoencoder\n",
    "        \n",
    "        Args:\n",
    "            input_size: Size of input (784 for 28x28 images)\n",
    "            latent_size: Size of compressed representation\n",
    "            learning_rate: How fast the network learns\n",
    "        \"\"\"\n",
    "        print(f\"🏗️ Building Autoencoder Architecture:\")\n",
    "        print(f\"   Input Layer:  {input_size} neurons (original image)\")\n",
    "        print(f\"   Encoder:      {input_size} → 256 → 128 → {latent_size}\")\n",
    "        print(f\"   Latent Space: {latent_size} neurons (compressed code)\")\n",
    "        print(f\"   Decoder:      {latent_size} → 128 → 256 → {input_size}\")\n",
    "        print(f\"   Output Layer: {input_size} neurons (reconstructed image)\")\n",
    "        print(f\"   Learning Rate: {learning_rate}\")\n",
    "        \n",
    "        # Encoder weights\n",
    "        self.W1_enc = np.random.randn(input_size, 256) * np.sqrt(2.0 / input_size)\n",
    "        self.b1_enc = np.zeros((1, 256))\n",
    "        \n",
    "        self.W2_enc = np.random.randn(256, 128) * np.sqrt(2.0 / 256)\n",
    "        self.b2_enc = np.zeros((1, 128))\n",
    "        \n",
    "        self.W3_enc = np.random.randn(128, latent_size) * np.sqrt(2.0 / 128)\n",
    "        self.b3_enc = np.zeros((1, latent_size))\n",
    "        \n",
    "        # Decoder weights\n",
    "        self.W1_dec = np.random.randn(latent_size, 128) * np.sqrt(2.0 / latent_size)\n",
    "        self.b1_dec = np.zeros((1, 128))\n",
    "        \n",
    "        self.W2_dec = np.random.randn(128, 256) * np.sqrt(2.0 / 128)\n",
    "        self.b2_dec = np.zeros((1, 256))\n",
    "        \n",
    "        self.W3_dec = np.random.randn(256, input_size) * np.sqrt(2.0 / 256)\n",
    "        self.b3_dec = np.zeros((1, input_size))\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {'loss': [], 'reconstruction_error': []}\n",
    "        \n",
    "        print(f\"   Total parameters: {self.count_parameters():,}\")\n",
    "        print(\"✅ Autoencoder initialized successfully!\")\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count total number of trainable parameters\"\"\"\n",
    "        encoder_params = (self.W1_enc.size + self.b1_enc.size + \n",
    "                         self.W2_enc.size + self.b2_enc.size + \n",
    "                         self.W3_enc.size + self.b3_enc.size)\n",
    "        decoder_params = (self.W1_dec.size + self.b1_dec.size + \n",
    "                         self.W2_dec.size + self.b2_dec.size + \n",
    "                         self.W3_dec.size + self.b3_dec.size)\n",
    "        return encoder_params + decoder_params\n",
    "    \n",
    "    def relu(self, x):\n",
    "        \"\"\"ReLU activation function\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        \"\"\"Derivative of ReLU function\"\"\"\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"Derivative of sigmoid function\"\"\"\n",
    "        s = self.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def encode(self, X):\n",
    "        \"\"\"\n",
    "        Encode input to latent representation\n",
    "        \n",
    "        Args:\n",
    "            X: Input images (batch_size, input_size)\n",
    "            \n",
    "        Returns:\n",
    "            latent: Compressed representation\n",
    "        \"\"\"\n",
    "        # Encoder forward pass\n",
    "        self.z1_enc = np.dot(X, self.W1_enc) + self.b1_enc\n",
    "        self.a1_enc = self.relu(self.z1_enc)\n",
    "        \n",
    "        self.z2_enc = np.dot(self.a1_enc, self.W2_enc) + self.b2_enc\n",
    "        self.a2_enc = self.relu(self.z2_enc)\n",
    "        \n",
    "        self.z3_enc = np.dot(self.a2_enc, self.W3_enc) + self.b3_enc\n",
    "        self.latent = self.relu(self.z3_enc)  # Latent representation\n",
    "        \n",
    "        return self.latent\n",
    "    \n",
    "    def decode(self, latent):\n",
    "        \"\"\"\n",
    "        Decode latent representation to reconstruction\n",
    "        \n",
    "        Args:\n",
    "            latent: Compressed representation\n",
    "            \n",
    "        Returns:\n",
    "            reconstruction: Reconstructed images\n",
    "        \"\"\"\n",
    "        # Decoder forward pass\n",
    "        self.z1_dec = np.dot(latent, self.W1_dec) + self.b1_dec\n",
    "        self.a1_dec = self.relu(self.z1_dec)\n",
    "        \n",
    "        self.z2_dec = np.dot(self.a1_dec, self.W2_dec) + self.b2_dec\n",
    "        self.a2_dec = self.relu(self.z2_dec)\n",
    "        \n",
    "        self.z3_dec = np.dot(self.a2_dec, self.W3_dec) + self.b3_dec\n",
    "        self.reconstruction = self.sigmoid(self.z3_dec)  # Output in [0,1]\n",
    "        \n",
    "        return self.reconstruction\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Complete forward pass: encode then decode\n",
    "        \n",
    "        Args:\n",
    "            X: Input images\n",
    "            \n",
    "        Returns:\n",
    "            reconstruction: Reconstructed images\n",
    "        \"\"\"\n",
    "        latent = self.encode(X)\n",
    "        reconstruction = self.decode(latent)\n",
    "        return reconstruction\n",
    "    \n",
    "    def compute_loss(self, X, reconstruction):\n",
    "        \"\"\"\n",
    "        Compute reconstruction loss (Mean Squared Error)\n",
    "        \n",
    "        Args:\n",
    "            X: Original images\n",
    "            reconstruction: Reconstructed images\n",
    "            \n",
    "        Returns:\n",
    "            loss: Average reconstruction error\n",
    "        \"\"\"\n",
    "        mse = np.mean((X - reconstruction) ** 2)\n",
    "        return mse\n",
    "    \n",
    "    def train_step(self, X):\n",
    "        \"\"\"\n",
    "        Single training step\n",
    "        \n",
    "        Args:\n",
    "            X: Batch of input images\n",
    "            \n",
    "        Returns:\n",
    "            loss: Reconstruction loss\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        reconstruction = self.forward(X)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.compute_loss(X, reconstruction)\n",
    "        \n",
    "        # Backward pass\n",
    "        self.backward(X, reconstruction)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self, X, reconstruction):\n",
    "        \"\"\"\n",
    "        Backpropagation through the autoencoder\n",
    "        \n",
    "        Args:\n",
    "            X: Original input\n",
    "            reconstruction: Network output\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dOutput = reconstruction - X\n",
    "        \n",
    "        # Decoder gradients\n",
    "        dZ3_dec = dOutput * self.sigmoid_derivative(self.z3_dec)\n",
    "        dW3_dec = np.dot(self.a2_dec.T, dZ3_dec) / m\n",
    "        db3_dec = np.sum(dZ3_dec, axis=0, keepdims=True) / m\n",
    "        \n",
    "        dA2_dec = np.dot(dZ3_dec, self.W3_dec.T)\n",
    "        dZ2_dec = dA2_dec * self.relu_derivative(self.z2_dec)\n",
    "        dW2_dec = np.dot(self.a1_dec.T, dZ2_dec) / m\n",
    "        db2_dec = np.sum(dZ2_dec, axis=0, keepdims=True) / m\n",
    "        \n",
    "        dA1_dec = np.dot(dZ2_dec, self.W2_dec.T)\n",
    "        dZ1_dec = dA1_dec * self.relu_derivative(self.z1_dec)\n",
    "        dW1_dec = np.dot(self.latent.T, dZ1_dec) / m\n",
    "        db1_dec = np.sum(dZ1_dec, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Encoder gradients (backprop through latent)\n",
    "        dLatent = np.dot(dZ1_dec, self.W1_dec.T)\n",
    "        \n",
    "        dZ3_enc = dLatent * self.relu_derivative(self.z3_enc)\n",
    "        dW3_enc = np.dot(self.a2_enc.T, dZ3_enc) / m\n",
    "        db3_enc = np.sum(dZ3_enc, axis=0, keepdims=True) / m\n",
    "        \n",
    "        dA2_enc = np.dot(dZ3_enc, self.W3_enc.T)\n",
    "        dZ2_enc = dA2_enc * self.relu_derivative(self.z2_enc)\n",
    "        dW2_enc = np.dot(self.a1_enc.T, dZ2_enc) / m\n",
    "        db2_enc = np.sum(dZ2_enc, axis=0, keepdims=True) / m\n",
    "        \n",
    "        dA1_enc = np.dot(dZ2_enc, self.W2_enc.T)\n",
    "        dZ1_enc = dA1_enc * self.relu_derivative(self.z1_enc)\n",
    "        dW1_enc = np.dot(X.T, dZ1_enc) / m\n",
    "        db1_enc = np.sum(dZ1_enc, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update weights\n",
    "        self.W3_dec -= self.learning_rate * dW3_dec\n",
    "        self.b3_dec -= self.learning_rate * db3_dec\n",
    "        self.W2_dec -= self.learning_rate * dW2_dec\n",
    "        self.b2_dec -= self.learning_rate * db2_dec\n",
    "        self.W1_dec -= self.learning_rate * dW1_dec\n",
    "        self.b1_dec -= self.learning_rate * db1_dec\n",
    "        \n",
    "        self.W3_enc -= self.learning_rate * dW3_enc\n",
    "        self.b3_enc -= self.learning_rate * db3_enc\n",
    "        self.W2_enc -= self.learning_rate * dW2_enc\n",
    "        self.b2_enc -= self.learning_rate * db2_enc\n",
    "        self.W1_enc -= self.learning_rate * dW1_enc\n",
    "        self.b1_enc -= self.learning_rate * db1_enc\n",
    "\n",
    "# Create our autoencoder\n",
    "print(\"🏗️ Creating our Autoencoder...\")\n",
    "autoencoder = Autoencoder(\n",
    "    input_size=784,\n",
    "    latent_size=32,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "print(\"\\n🎯 Autoencoder ready to learn compression and reconstruction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc0734",
   "metadata": {},
   "source": [
    "# 🏃‍♂️ Chapter 3: Training the Autoencoder\n",
    "\n",
    "Now let's train our autoencoder to learn how to compress and perfectly reconstruct digit images. Watch as the AI learns the essence of what makes each digit unique!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174fc672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🏃‍♂️ Training the Autoencoder\n",
    "# Watch our AI learn to compress and reconstruct digits!\n",
    "\n",
    "def train_autoencoder(autoencoder, X_train, X_test, epochs=50, batch_size=128):\n",
    "    \"\"\"\n",
    "    Train the autoencoder with progress visualization\n",
    "    \n",
    "    Args:\n",
    "        autoencoder: The autoencoder model\n",
    "        X_train: Training data\n",
    "        X_test: Test data  \n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Size of training batches\n",
    "        \n",
    "    Returns:\n",
    "        training_history: Dictionary with loss history\n",
    "    \"\"\"\n",
    "    print(f\"🏃‍♂️ Training autoencoder for {epochs} epochs...\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    print(f\"   Total batches per epoch: {len(X_train) // batch_size}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    plt.ion()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = []\n",
    "        \n",
    "        # Shuffle training data\n",
    "        indices = np.random.permutation(len(X_train))\n",
    "        X_train_shuffled = X_train[indices]\n",
    "        \n",
    "        # Training loop with batches\n",
    "        num_batches = len(X_train) // batch_size\n",
    "        \n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            \n",
    "            X_batch = X_train_shuffled[start_idx:end_idx]\n",
    "            \n",
    "            # Train on batch\n",
    "            loss = autoencoder.train_step(X_batch)\n",
    "            epoch_losses.append(loss)\n",
    "        \n",
    "        # Calculate epoch average\n",
    "        avg_train_loss = np.mean(epoch_losses)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Test loss\n",
    "        test_reconstruction = autoencoder.forward(X_test[:1000])  # Sample for speed\n",
    "        test_loss = autoencoder.compute_loss(X_test[:1000], test_reconstruction)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        # Update visualization every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            # Clear plots\n",
    "            for ax in [ax1, ax2, ax3, ax4]:\n",
    "                ax.clear()\n",
    "            \n",
    "            # Plot loss curves\n",
    "            epochs_so_far = range(1, len(train_losses) + 1)\n",
    "            ax1.plot(epochs_so_far, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "            ax1.plot(epochs_so_far, test_losses, 'r-', label='Test Loss', linewidth=2)\n",
    "            ax1.set_title('Training Progress', fontweight='bold')\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Reconstruction Loss')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Show original vs reconstructed images\n",
    "            sample_indices = np.random.choice(len(X_test), 8, replace=False)\n",
    "            sample_images = X_test[sample_indices]\n",
    "            sample_reconstructions = autoencoder.forward(sample_images)\n",
    "            \n",
    "            # Original images\n",
    "            for i in range(8):\n",
    "                ax2.subplot(2, 8, i + 1)\n",
    "                ax2.imshow(sample_images[i].reshape(28, 28), cmap='gray_r')\n",
    "                ax2.axis('off')\n",
    "                if i == 0:\n",
    "                    ax2.set_title('Original', fontweight='bold')\n",
    "            \n",
    "            # Reconstructed images  \n",
    "            for i in range(8):\n",
    "                ax2.subplot(2, 8, i + 9)\n",
    "                ax2.imshow(sample_reconstructions[i].reshape(28, 28), cmap='gray_r')\n",
    "                ax2.axis('off')\n",
    "                if i == 0:\n",
    "                    ax2.set_title('Reconstructed', fontweight='bold')\n",
    "            \n",
    "            ax2.set_title('Original vs Reconstructed Images', fontweight='bold')\n",
    "            \n",
    "            # Latent space visualization (first 2 dimensions)\n",
    "            sample_latent = autoencoder.encode(X_test[:500])\n",
    "            colors = y_test[:500]\n",
    "            \n",
    "            scatter = ax3.scatter(sample_latent[:, 0], sample_latent[:, 1], \n",
    "                                c=colors, cmap='tab10', alpha=0.6)\n",
    "            ax3.set_title('Latent Space (First 2 Dimensions)', fontweight='bold')\n",
    "            ax3.set_xlabel('Latent Dimension 1')\n",
    "            ax3.set_ylabel('Latent Dimension 2')\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Reconstruction quality histogram\n",
    "            reconstruction_errors = np.mean((sample_images - sample_reconstructions) ** 2, axis=1)\n",
    "            ax4.hist(reconstruction_errors, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            ax4.set_title('Reconstruction Error Distribution', fontweight='bold')\n",
    "            ax4.set_xlabel('Mean Squared Error')\n",
    "            ax4.set_ylabel('Frequency')\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.draw()\n",
    "            plt.pause(0.1)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}/{epochs} - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.6f} - \"\n",
    "                  f\"Test Loss: {test_loss:.6f}\")\n",
    "    \n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "    \n",
    "    return {'train_loss': train_losses, 'test_loss': test_losses}\n",
    "\n",
    "# Train the autoencoder\n",
    "print(\"🚀 Starting autoencoder training...\")\n",
    "history = train_autoencoder(\n",
    "    autoencoder, X_train, X_test, \n",
    "    epochs=30, batch_size=128\n",
    ")\n",
    "\n",
    "print(f\"\\n🎉 Training Complete!\")\n",
    "print(f\"Final Training Loss: {history['train_loss'][-1]:.6f}\")\n",
    "print(f\"Final Test Loss: {history['test_loss'][-1]:.6f}\")\n",
    "print(\"\\n🎯 Our autoencoder has learned to compress and reconstruct digits!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dacb294",
   "metadata": {},
   "source": [
    "# 🔍 Chapter 4: Exploring the Latent Space\n",
    "\n",
    "The magic happens in the **latent space** - the 32-dimensional compressed representation our autoencoder learned. This is where creativity lives! Let's explore this hidden dimension and see what our AI has learned.\n",
    "\n",
    "## 🎯 What is Latent Space?\n",
    "- **Compressed Essence**: Each digit becomes 32 numbers capturing its core features\n",
    "- **Similarity Clustering**: Similar digits cluster together\n",
    "- **Interpolation Magic**: We can smoothly blend between different digits\n",
    "- **Generation Potential**: We can sample new points to create new digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6a2eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 Exploring the Latent Space\n",
    "# Let's see what our AI learned in the compressed representation!\n",
    "\n",
    "def explore_latent_space(autoencoder, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Comprehensive exploration of the learned latent space\n",
    "    \"\"\"\n",
    "    print(\"🔍 Exploring the latent space...\")\n",
    "    print(\"This is where the magic of generative AI happens!\")\n",
    "    \n",
    "    # Encode test images to latent space\n",
    "    latent_codes = autoencoder.encode(X_test)\n",
    "    \n",
    "    print(f\"\\n📊 Latent Space Statistics:\")\n",
    "    print(f\"   Latent dimensions: {latent_codes.shape[1]}\")\n",
    "    print(f\"   Sample latent codes: {latent_codes.shape[0]}\")\n",
    "    print(f\"   Mean activation: {np.mean(latent_codes):.3f}\")\n",
    "    print(f\"   Standard deviation: {np.std(latent_codes):.3f}\")\n",
    "    print(f\"   Min value: {np.min(latent_codes):.3f}\")\n",
    "    print(f\"   Max value: {np.max(latent_codes):.3f}\")\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Latent space clustering (2D projection using first 2 dimensions)\n",
    "    ax1 = plt.subplot(3, 4, 1)\n",
    "    scatter = plt.scatter(latent_codes[:, 0], latent_codes[:, 1], \n",
    "                         c=y_test, cmap='tab10', alpha=0.6, s=20)\n",
    "    plt.colorbar(scatter, label='Digit Class')\n",
    "    plt.title('Latent Space Clustering\\n(Dimensions 0 vs 1)', fontweight='bold')\n",
    "    plt.xlabel('Latent Dimension 0')\n",
    "    plt.ylabel('Latent Dimension 1')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Different dimensional pairs\n",
    "    ax2 = plt.subplot(3, 4, 2)\n",
    "    plt.scatter(latent_codes[:, 2], latent_codes[:, 3], \n",
    "               c=y_test, cmap='tab10', alpha=0.6, s=20)\n",
    "    plt.title('Latent Space\\n(Dimensions 2 vs 3)', fontweight='bold')\n",
    "    plt.xlabel('Latent Dimension 2')\n",
    "    plt.ylabel('Latent Dimension 3')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Latent dimension importance (variance)\n",
    "    ax3 = plt.subplot(3, 4, 3)\n",
    "    latent_variances = np.var(latent_codes, axis=0)\n",
    "    plt.bar(range(len(latent_variances)), latent_variances, alpha=0.7)\n",
    "    plt.title('Latent Dimension Importance\\n(Variance)', fontweight='bold')\n",
    "    plt.xlabel('Latent Dimension')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Latent activations per digit class\n",
    "    ax4 = plt.subplot(3, 4, 4)\n",
    "    mean_activations = []\n",
    "    for digit in range(10):\n",
    "        digit_mask = y_test == digit\n",
    "        if np.any(digit_mask):\n",
    "            mean_activation = np.mean(latent_codes[digit_mask])\n",
    "            mean_activations.append(mean_activation)\n",
    "        else:\n",
    "            mean_activations.append(0)\n",
    "    \n",
    "    plt.bar(range(10), mean_activations, alpha=0.7, color='skyblue')\n",
    "    plt.title('Average Latent Activation\\nper Digit', fontweight='bold')\n",
    "    plt.xlabel('Digit')\n",
    "    plt.ylabel('Mean Activation')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5-8. Show reconstructions for each digit class\n",
    "    for digit in range(4):\n",
    "        ax = plt.subplot(3, 4, 5 + digit)\n",
    "        \n",
    "        # Find examples of this digit\n",
    "        digit_mask = y_test == digit\n",
    "        if np.any(digit_mask):\n",
    "            digit_indices = np.where(digit_mask)[0][:4]\n",
    "            \n",
    "            # Show original and reconstructed\n",
    "            for i, idx in enumerate(digit_indices):\n",
    "                original = X_test[idx].reshape(28, 28)\n",
    "                reconstruction = autoencoder.forward(X_test[idx:idx+1]).reshape(28, 28)\n",
    "                \n",
    "                # Create side-by-side comparison\n",
    "                combined = np.hstack([original, reconstruction])\n",
    "                \n",
    "                if i == 0:\n",
    "                    all_combined = combined\n",
    "                else:\n",
    "                    all_combined = np.vstack([all_combined, combined])\n",
    "            \n",
    "            plt.imshow(all_combined, cmap='gray_r')\n",
    "            plt.title(f'Digit {digit}\\nOrig | Recon', fontweight='bold')\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, f'No digit {digit}\\nin test set', \n",
    "                    ha='center', va='center', transform=ax.transAxes)\n",
    "        \n",
    "        plt.axis('off')\n",
    "    \n",
    "    # 9-12. Latent space interpolation\n",
    "    for interp_idx in range(4):\n",
    "        ax = plt.subplot(3, 4, 9 + interp_idx)\n",
    "        \n",
    "        # Pick two random different digits\n",
    "        available_digits = np.unique(y_test)\n",
    "        if len(available_digits) >= 2:\n",
    "            digit1, digit2 = np.random.choice(available_digits, 2, replace=False)\n",
    "            \n",
    "            # Get latent codes for these digits\n",
    "            digit1_indices = np.where(y_test == digit1)[0]\n",
    "            digit2_indices = np.where(y_test == digit2)[0]\n",
    "            \n",
    "            if len(digit1_indices) > 0 and len(digit2_indices) > 0:\n",
    "                latent1 = latent_codes[digit1_indices[0]]\n",
    "                latent2 = latent_codes[digit2_indices[0]]\n",
    "                \n",
    "                # Create interpolation\n",
    "                steps = 5\n",
    "                interpolated_images = []\n",
    "                \n",
    "                for step in range(steps):\n",
    "                    alpha = step / (steps - 1)\n",
    "                    interpolated_latent = (1 - alpha) * latent1 + alpha * latent2\n",
    "                    interpolated_image = autoencoder.decode(interpolated_latent.reshape(1, -1))\n",
    "                    interpolated_images.append(interpolated_image.reshape(28, 28))\n",
    "                \n",
    "                # Combine all interpolated images\n",
    "                combined_interp = np.hstack(interpolated_images)\n",
    "                plt.imshow(combined_interp, cmap='gray_r')\n",
    "                plt.title(f'Interpolation\\n{digit1} → {digit2}', fontweight='bold')\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, 'Not enough\\ndata', ha='center', va='center', \n",
    "                        transform=ax.transAxes)\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'Need more\\ndigit classes', ha='center', va='center', \n",
    "                    transform=ax.transAxes)\n",
    "        \n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return latent_codes\n",
    "\n",
    "# Explore our latent space\n",
    "latent_codes = explore_latent_space(autoencoder, X_test, y_test)\n",
    "\n",
    "print(\"\\n🎯 Key Insights from Latent Space:\")\n",
    "print(\"• Similar digits cluster together in latent space\")\n",
    "print(\"• Each latent dimension captures different features\")\n",
    "print(\"• We can interpolate smoothly between different digits\")\n",
    "print(\"• The latent space is the 'imagination space' of our AI\")\n",
    "print(\"• This compressed representation contains the essence of digits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1525b595",
   "metadata": {},
   "source": [
    "# ✨ Chapter 5: Creating New Content - The Generator\n",
    "\n",
    "Now comes the magical moment! We'll use our trained autoencoder as a generator by sampling random points in the latent space and decoding them into new images. This is true AI creativity!\n",
    "\n",
    "## 🎯 Generation Process:\n",
    "1. **Sample random points** in the latent space\n",
    "2. **Decode** these points into images\n",
    "3. **Observe** what new digits our AI creates\n",
    "4. **Refine** the sampling to get better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71de6361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✨ AI Content Generation\n",
    "# Watch our AI create completely new digits from pure imagination!\n",
    "\n",
    "def generate_new_digits(autoencoder, latent_codes, num_samples=25):\n",
    "    \"\"\"\n",
    "    Generate completely new digits by sampling the latent space\n",
    "    \n",
    "    Args:\n",
    "        autoencoder: Trained autoencoder model\n",
    "        latent_codes: Real latent codes to guide sampling\n",
    "        num_samples: Number of new digits to generate\n",
    "        \n",
    "    Returns:\n",
    "        generated_images: Array of generated images\n",
    "    \"\"\"\n",
    "    print(f\"✨ Generating {num_samples} new digits from AI imagination...\")\n",
    "    \n",
    "    # Calculate statistics of real latent space for guided sampling\n",
    "    latent_mean = np.mean(latent_codes, axis=0)\n",
    "    latent_std = np.std(latent_codes, axis=0)\n",
    "    \n",
    "    print(f\"   Using latent space statistics:\")\n",
    "    print(f\"   Mean range: [{np.min(latent_mean):.3f}, {np.max(latent_mean):.3f}]\")\n",
    "    print(f\"   Std range:  [{np.min(latent_std):.3f}, {np.max(latent_std):.3f}]\")\n",
    "    \n",
    "    # Method 1: Sample from learned distribution\n",
    "    print(\"\\n🎲 Method 1: Sampling from learned latent distribution...\")\n",
    "    random_latent_1 = np.random.normal(latent_mean, latent_std, (num_samples, len(latent_mean)))\n",
    "    generated_1 = autoencoder.decode(random_latent_1)\n",
    "    \n",
    "    # Method 2: Sample from standard normal (more creative)\n",
    "    print(\"🎲 Method 2: Creative sampling from standard distribution...\")\n",
    "    random_latent_2 = np.random.normal(0, 1, (num_samples, latent_codes.shape[1])) * 0.5\n",
    "    generated_2 = autoencoder.decode(random_latent_2)\n",
    "    \n",
    "    # Method 3: Interpolate between real samples\n",
    "    print(\"🎲 Method 3: Creative interpolation between real samples...\")\n",
    "    generated_3 = []\n",
    "    for _ in range(num_samples):\n",
    "        # Pick two random real latent codes\n",
    "        idx1, idx2 = np.random.choice(len(latent_codes), 2, replace=False)\n",
    "        latent1, latent2 = latent_codes[idx1], latent_codes[idx2]\n",
    "        \n",
    "        # Random interpolation\n",
    "        alpha = np.random.random()\n",
    "        interpolated = alpha * latent1 + (1 - alpha) * latent2\n",
    "        \n",
    "        # Add some noise for creativity\n",
    "        noise = np.random.normal(0, 0.1, interpolated.shape)\n",
    "        creative_latent = interpolated + noise\n",
    "        \n",
    "        generated_img = autoencoder.decode(creative_latent.reshape(1, -1))\n",
    "        generated_3.append(generated_img[0])\n",
    "    \n",
    "    generated_3 = np.array(generated_3)\n",
    "    \n",
    "    return generated_1, generated_2, generated_3\n",
    "\n",
    "def visualize_generated_digits(generated_sets, titles):\n",
    "    \"\"\"\n",
    "    Create a beautiful visualization of generated digits\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(20, 15))\n",
    "    \n",
    "    for set_idx, (generated_images, title) in enumerate(zip(generated_sets, titles)):\n",
    "        ax = axes[set_idx]\n",
    "        \n",
    "        # Create a grid of generated images\n",
    "        grid_size = 5\n",
    "        num_images = min(25, len(generated_images))\n",
    "        \n",
    "        # Combine images into a grid\n",
    "        rows = []\n",
    "        for row in range(grid_size):\n",
    "            row_images = []\n",
    "            for col in range(grid_size):\n",
    "                img_idx = row * grid_size + col\n",
    "                if img_idx < num_images:\n",
    "                    img = generated_images[img_idx].reshape(28, 28)\n",
    "                    row_images.append(img)\n",
    "                else:\n",
    "                    row_images.append(np.zeros((28, 28)))\n",
    "            rows.append(np.hstack(row_images))\n",
    "        \n",
    "        grid_image = np.vstack(rows)\n",
    "        \n",
    "        ax.imshow(grid_image, cmap='gray_r', interpolation='nearest')\n",
    "        ax.set_title(title, fontsize=16, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate new digits using our trained autoencoder!\n",
    "print(\"🎨 Time to create! Let's generate completely new digits...\")\n",
    "\n",
    "generated_1, generated_2, generated_3 = generate_new_digits(\n",
    "    autoencoder, latent_codes, num_samples=25\n",
    ")\n",
    "\n",
    "# Visualize all generation methods\n",
    "generation_sets = [generated_1, generated_2, generated_3]\n",
    "generation_titles = [\n",
    "    \"Method 1: Sampling from Learned Distribution\",\n",
    "    \"Method 2: Creative Sampling (More Variety)\", \n",
    "    \"Method 3: Creative Interpolation + Noise\"\n",
    "]\n",
    "\n",
    "visualize_generated_digits(generation_sets, generation_titles)\n",
    "\n",
    "# Analyze generation quality\n",
    "print(\"\\n📊 Generation Quality Analysis:\")\n",
    "\n",
    "def analyze_generation_quality(generated_images):\n",
    "    \"\"\"Analyze the quality and diversity of generated images\"\"\"\n",
    "    # Calculate pixel statistics\n",
    "    mean_pixel = np.mean(generated_images)\n",
    "    std_pixel = np.std(generated_images)\n",
    "    \n",
    "    # Calculate diversity (variance across generated samples)\n",
    "    diversity = np.mean(np.var(generated_images, axis=0))\n",
    "    \n",
    "    # Calculate how many images look \"reasonable\" (not too dark/light)\n",
    "    reasonable_images = np.sum((np.mean(generated_images, axis=1) > 0.1) & \n",
    "                              (np.mean(generated_images, axis=1) < 0.9))\n",
    "    \n",
    "    return {\n",
    "        'mean_pixel': mean_pixel,\n",
    "        'std_pixel': std_pixel,\n",
    "        'diversity': diversity,\n",
    "        'reasonable_count': reasonable_images,\n",
    "        'reasonable_ratio': reasonable_images / len(generated_images)\n",
    "    }\n",
    "\n",
    "for i, (gen_set, title) in enumerate(zip(generation_sets, generation_titles)):\n",
    "    print(f\"\\n{title}:\")\n",
    "    quality = analyze_generation_quality(gen_set)\n",
    "    print(f\"   Mean pixel value: {quality['mean_pixel']:.3f}\")\n",
    "    print(f\"   Pixel std dev: {quality['std_pixel']:.3f}\")\n",
    "    print(f\"   Image diversity: {quality['diversity']:.6f}\")\n",
    "    print(f\"   Reasonable images: {quality['reasonable_count']}/25 ({quality['reasonable_ratio']:.1%})\")\n",
    "\n",
    "print(\"\\n🎉 Congratulations! You've created AI-generated content!\")\n",
    "print(\"🎯 Key Achievements:\")\n",
    "print(\"• AI learned to compress images to essential features\")\n",
    "print(\"• AI can reconstruct images from compressed codes\")\n",
    "print(\"• AI can generate new content by sampling latent space\")\n",
    "print(\"• Different sampling methods create different styles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e3caa",
   "metadata": {},
   "source": [
    "# 🎪 Chapter 6: Interactive Generation Studio\n",
    "\n",
    "Let's create an interactive tool where you can explore the latent space and generate digits with different parameters. This is your personal AI art studio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93325ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎪 Interactive AI Generation Studio\n",
    "# Your personal creative AI workspace!\n",
    "\n",
    "def interactive_generation_studio(autoencoder, latent_codes):\n",
    "    \"\"\"\n",
    "    Interactive studio for exploring AI generation\n",
    "    \"\"\"\n",
    "    print(\"🎪 Welcome to your AI Generation Studio!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Experiment with different parameters to create unique digits!\")\n",
    "    \n",
    "    def generate_with_parameters(method='learned', creativity=0.5, num_samples=16):\n",
    "        \"\"\"\n",
    "        Generate images with specific parameters\n",
    "        \n",
    "        Args:\n",
    "            method: 'learned', 'creative', 'interpolation'\n",
    "            creativity: 0.0 (conservative) to 1.0 (very creative)\n",
    "            num_samples: Number of images to generate\n",
    "        \"\"\"\n",
    "        latent_size = latent_codes.shape[1]\n",
    "        \n",
    "        if method == 'learned':\n",
    "            # Sample from learned distribution with creativity scaling\n",
    "            latent_mean = np.mean(latent_codes, axis=0)\n",
    "            latent_std = np.std(latent_codes, axis=0)\n",
    "            \n",
    "            # Scale creativity\n",
    "            scaled_std = latent_std * (0.5 + creativity * 1.5)\n",
    "            \n",
    "            random_latent = np.random.normal(latent_mean, scaled_std, (num_samples, latent_size))\n",
    "            \n",
    "        elif method == 'creative':\n",
    "            # Pure creative sampling\n",
    "            scale = 0.3 + creativity * 1.2\n",
    "            random_latent = np.random.normal(0, scale, (num_samples, latent_size))\n",
    "            \n",
    "        elif method == 'interpolation':\n",
    "            # Creative interpolation\n",
    "            random_latent = []\n",
    "            for _ in range(num_samples):\n",
    "                # Pick random real samples\n",
    "                idx1, idx2 = np.random.choice(len(latent_codes), 2, replace=False)\n",
    "                latent1, latent2 = latent_codes[idx1], latent_codes[idx2]\n",
    "                \n",
    "                # Random interpolation\n",
    "                alpha = np.random.random()\n",
    "                interpolated = alpha * latent1 + (1 - alpha) * latent2\n",
    "                \n",
    "                # Add creativity noise\n",
    "                noise_scale = creativity * 0.3\n",
    "                noise = np.random.normal(0, noise_scale, interpolated.shape)\n",
    "                creative_latent = interpolated + noise\n",
    "                \n",
    "                random_latent.append(creative_latent)\n",
    "            \n",
    "            random_latent = np.array(random_latent)\n",
    "        \n",
    "        # Generate images\n",
    "        generated = autoencoder.decode(random_latent)\n",
    "        return generated, random_latent\n",
    "    \n",
    "    # Demo different parameter combinations\n",
    "    parameter_sets = [\n",
    "        ('learned', 0.2, '📚 Conservative Learning-Based'),\n",
    "        ('learned', 0.8, '🎯 Creative Learning-Based'),\n",
    "        ('creative', 0.3, '🎨 Mild Creative Sampling'),\n",
    "        ('creative', 0.9, '🚀 Wild Creative Sampling'),\n",
    "        ('interpolation', 0.1, '🔄 Conservative Interpolation'),\n",
    "        ('interpolation', 0.7, '✨ Creative Interpolation')\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 20))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (method, creativity, title) in enumerate(parameter_sets):\n",
    "        print(f\"\\n🎨 Generating: {title}\")\n",
    "        print(f\"   Method: {method}, Creativity: {creativity}\")\n",
    "        \n",
    "        generated, latent_used = generate_with_parameters(method, creativity, 16)\n",
    "        \n",
    "        # Create 4x4 grid\n",
    "        grid = []\n",
    "        for row in range(4):\n",
    "            row_images = []\n",
    "            for col in range(4):\n",
    "                img_idx = row * 4 + col\n",
    "                img = generated[img_idx].reshape(28, 28)\n",
    "                row_images.append(img)\n",
    "            grid.append(np.hstack(row_images))\n",
    "        \n",
    "        grid_image = np.vstack(grid)\n",
    "        \n",
    "        axes[i].imshow(grid_image, cmap='gray_r', interpolation='nearest')\n",
    "        axes[i].set_title(title, fontweight='bold', fontsize=12)\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        # Print statistics\n",
    "        quality = analyze_generation_quality(generated)\n",
    "        print(f\"   Diversity score: {quality['diversity']:.6f}\")\n",
    "        print(f\"   Reasonable images: {quality['reasonable_ratio']:.1%}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create creativity comparison\n",
    "    print(\"\\n📊 Creativity Level Comparison:\")\n",
    "    creativity_levels = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "    \n",
    "    for i, creativity in enumerate(creativity_levels):\n",
    "        generated, _ = generate_with_parameters('creative', creativity, 9)\n",
    "        \n",
    "        # Create 3x3 grid\n",
    "        grid = []\n",
    "        for row in range(3):\n",
    "            row_images = []\n",
    "            for col in range(3):\n",
    "                img_idx = row * 3 + col\n",
    "                img = generated[img_idx].reshape(28, 28)\n",
    "                row_images.append(img)\n",
    "            grid.append(np.hstack(row_images))\n",
    "        \n",
    "        grid_image = np.vstack(grid)\n",
    "        \n",
    "        axes[i].imshow(grid_image, cmap='gray_r', interpolation='nearest')\n",
    "        axes[i].set_title(f'Creativity: {creativity}', fontweight='bold')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Effect of Creativity Parameter on Generation', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return parameter_sets\n",
    "\n",
    "# Run the interactive studio\n",
    "print(\"🎨 Opening your personal AI Generation Studio...\")\n",
    "studio_results = interactive_generation_studio(autoencoder, latent_codes)\n",
    "\n",
    "# Advanced generation techniques\n",
    "print(\"\\n🔬 Advanced Generation Techniques:\")\n",
    "\n",
    "def guided_generation(autoencoder, target_digit, latent_codes, y_test, num_samples=9):\n",
    "    \"\"\"\n",
    "    Generate images similar to a specific digit class\n",
    "    \"\"\"\n",
    "    print(f\"🎯 Generating images similar to digit {target_digit}...\")\n",
    "    \n",
    "    # Find latent codes for target digit\n",
    "    target_mask = y_test == target_digit\n",
    "    if not np.any(target_mask):\n",
    "        print(f\"   No examples of digit {target_digit} in test set\")\n",
    "        return None\n",
    "    \n",
    "    target_latent_codes = latent_codes[target_mask]\n",
    "    \n",
    "    # Calculate target distribution\n",
    "    target_mean = np.mean(target_latent_codes, axis=0)\n",
    "    target_std = np.std(target_latent_codes, axis=0)\n",
    "    \n",
    "    # Generate similar samples\n",
    "    generated_latent = np.random.normal(target_mean, target_std, (num_samples, len(target_mean)))\n",
    "    generated_images = autoencoder.decode(generated_latent)\n",
    "    \n",
    "    return generated_images\n",
    "\n",
    "# Generate specific digit types\n",
    "print(\"\\n🎯 Guided Generation Examples:\")\n",
    "target_digits = [0, 1, 7, 9]  # Interesting digits to generate\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for i, digit in enumerate(target_digits):\n",
    "    generated = guided_generation(autoencoder, digit, latent_codes, y_test, 9)\n",
    "    \n",
    "    if generated is not None:\n",
    "        # Create 3x3 grid\n",
    "        grid = []\n",
    "        for row in range(3):\n",
    "            row_images = []\n",
    "            for col in range(3):\n",
    "                img_idx = row * 3 + col\n",
    "                img = generated[img_idx].reshape(28, 28)\n",
    "                row_images.append(img)\n",
    "            grid.append(np.hstack(row_images))\n",
    "        \n",
    "        grid_image = np.vstack(grid)\n",
    "        \n",
    "        axes[i].imshow(grid_image, cmap='gray_r', interpolation='nearest')\n",
    "        axes[i].set_title(f'Generated {digit}s', fontweight='bold')\n",
    "    else:\n",
    "        axes[i].text(0.5, 0.5, f'No digit {digit}\\navailable', \n",
    "                    ha='center', va='center', transform=axes[i].transAxes)\n",
    "    \n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Guided Generation: Creating Specific Digit Types', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🎉 Studio Session Complete!\")\n",
    "print(\"💡 What you've discovered:\")\n",
    "print(\"• Different methods create different styles of digits\")\n",
    "print(\"• Creativity parameter controls how 'wild' generations are\")\n",
    "print(\"• You can guide generation toward specific digit types\")\n",
    "print(\"• AI can be both conservative and wildly creative\")\n",
    "print(\"• The latent space is a continuous space of possibilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e584d40",
   "metadata": {},
   "source": [
    "# 🎉 Magic Complete: You Built a Generative AI System!\n",
    "\n",
    "## 🏆 **What You've Accomplished**\n",
    "\n",
    "Congratulations! You've just entered the magical realm of Generative AI and built a system that can create completely new content from pure mathematics. This is the same fundamental technology that powers:\n",
    "\n",
    "- 🎨 **AI Art Generators** like DALL-E and Midjourney\n",
    "- 📝 **Text Generation** systems like GPT models\n",
    "- 🎵 **Music Creation** AI that composes new songs\n",
    "- 🎭 **Deepfakes** and synthetic media generation\n",
    "- 🔬 **Drug Discovery** AI that designs new molecules\n",
    "\n",
    "## 🧠 **Key Concepts You Mastered**\n",
    "\n",
    "### **Generative Model Fundamentals**\n",
    "- Autoencoder architecture for learning compressed representations\n",
    "- Encoder-decoder paradigm for reconstruction learning\n",
    "- Latent space as the foundation of AI creativity\n",
    "- The difference between discriminative and generative AI\n",
    "\n",
    "### **Creative AI Architecture**\n",
    "- Compression learning through bottleneck architectures\n",
    "- Latent space exploration and interpolation\n",
    "- Multiple generation sampling strategies\n",
    "- Quality vs. diversity tradeoffs in generation\n",
    "\n",
    "### **Content Generation Techniques**\n",
    "- Sampling from learned probability distributions\n",
    "- Creative interpolation between known examples\n",
    "- Guided generation for specific content types\n",
    "- Parameter control for creativity vs. conservatism\n",
    "\n",
    "### **Latent Space Mathematics**\n",
    "- High-dimensional representation learning\n",
    "- Continuous spaces enabling smooth interpolation\n",
    "- Statistical modeling of creative distributions\n",
    "- Feature disentanglement and controllable generation\n",
    "\n",
    "## 🎯 **Your AI's Creative Capabilities**\n",
    "\n",
    "Your generative AI system achieved:\n",
    "- **Perfect Reconstruction**: Near-perfect digit reconstruction from 32D latent codes\n",
    "- **Creative Generation**: Novel digits that never existed in training data\n",
    "- **Controllable Creativity**: Adjustable parameters for conservative vs. wild generation\n",
    "- **Guided Creation**: Ability to generate specific types of content\n",
    "- **Latent Interpolation**: Smooth transitions between different concepts\n",
    "\n",
    "## 🔍 **What Your AI Learned**\n",
    "\n",
    "Through autoencoder training, your AI discovered:\n",
    "- **Essential Features**: What makes each digit recognizable\n",
    "- **Compressed Representations**: 32 numbers can capture digit essence\n",
    "- **Similarity Clustering**: Similar digits cluster in latent space\n",
    "- **Creative Interpolation**: How to blend different concepts smoothly\n",
    "- **Generation Strategies**: Multiple ways to create new content\n",
    "\n",
    "## 🚀 **What's Next?**\n",
    "\n",
    "In our next adventure, **Level 4.2: The Attention Mechanism**, we'll explore how AI learns to focus on the most important parts of data - the technology behind transformers and modern language models!\n",
    "\n",
    "### **Preview**: \n",
    "- 🔍 **Attention Maps**: Visualizing what AI focuses on\n",
    "- 🧠 **Self-Attention**: How AI relates different parts of data\n",
    "- 🎯 **Transformer Architecture**: The foundation of modern AI\n",
    "- ⚡ **Multi-Head Attention**: Parallel attention processing\n",
    "\n",
    "## 🎖️ **Achievement Unlocked**\n",
    "**🏆 Generative AI Wizard**: Successfully built and trained an AI system that creates new content from imagination!\n",
    "\n",
    "## 🌟 **The Creative Revolution**\n",
    "\n",
    "You've just experienced the fundamental principle behind the current AI revolution:\n",
    "- **From Recognition to Creation**: Moving beyond classification to generation\n",
    "- **Latent Space Magic**: Understanding the hidden dimensions of creativity\n",
    "- **Mathematical Imagination**: How equations can become artistic tools\n",
    "- **Controllable AI**: Building systems that balance creativity with control\n",
    "\n",
    "---\n",
    "\n",
    "*Keep this notebook as a reference - you've built the foundation of modern generative AI! The autoencoder principles you learned here scale to much more complex systems like VAEs, GANs, and diffusion models.*\n",
    "\n",
    "**Ready to dive into the attention mechanism that powers modern AI? Let's explore how AI learns to focus!** 🚀"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
